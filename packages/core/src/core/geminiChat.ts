/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

// DISCLAIMER: This is a copied version of https://github.com/googleapis/js-genai/blob/main/src/chats.ts with the intention of working around a key bug
// where function responses are not treated as "valid" responses: https://b.corp.google.com/issues/420354090

import {
  GenerateContentResponse,
  GenerateContentConfig,
  SendMessageParameters,
  createUserContent,
  Part,
  GenerateContentResponseUsageMetadata,
  Tool,
  ContentUnion,
} from '@google/genai';
import { Content, stripUIFieldsFromArray } from '../types/extendedContent.js';
import { retryWithBackoff } from '../utils/retry.js';
import { isFunctionResponse, hasFunctionCall } from '../utils/messageInspectors.js';
import { MESSAGE_ROLES } from '../config/messageRoles.js';
import { SceneType } from './sceneManager.js';
import { ContentGenerator, AuthType } from './contentGenerator.js';
import { Config } from '../config/config.js';
import { isDeepXQuotaError } from '../utils/quotaErrorDetection.js';
import {
  logApiRequest,
  logApiResponse,
  logApiError,
} from '../telemetry/loggers.js';
import {
  ApiErrorEvent,
  ApiRequestEvent,
  ApiResponseEvent,
  AgentContext,
} from '../telemetry/types.js';
import { DEFAULT_GEMINI_FLASH_MODEL, DEFAULT_GEMINI_MODEL } from '../config/models.js';
import { tokenUsageEventManager } from '../events/tokenUsageEvents.js';
import { realTimeTokenEventManager } from '../events/realTimeTokenEvents.js';
import { SessionManager } from '../services/sessionManager.js';

/**
 * Returns true if the response is valid, false otherwise.
 */
function isValidResponse(response: GenerateContentResponse): boolean {
  if (response.candidates === undefined || response.candidates.length === 0) {
    return false;
  }
  const content = response.candidates[0]?.content;
  if (content === undefined) {
    return false;
  }
  return isValidContent(content);
}

function isValidContent(content: Content): boolean {
  if (content.parts === undefined || content.parts.length === 0) {
    return false;
  }
  for (const part of content.parts) {
    if (part === undefined || Object.keys(part).length === 0) {
      return false;
    }
    if (!part.thought && part.text !== undefined && part.text === '') {
      return false;
    }
  }
  return true;
}

/**
 * Validates the history contains the correct roles.
 *
 * @throws Error if the history does not start with a user turn.
 * @throws Error if the history contains an invalid role.
 */
function validateHistory(history: Content[]) {
  for (const content of history) {
    if (content.role !== MESSAGE_ROLES.USER && content.role !== MESSAGE_ROLES.MODEL) {
      throw new Error(`Role must be user or model, but got ${content.role}.`);
    }
  }
}

/**
 * æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸º reasoningï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
 */
function isReasoningContent(content: Content | undefined): boolean {
  return !!(
    content &&
    content.role === 'model' &&
    content.parts &&
    content.parts.length > 0 &&
    'reasoning' in content.parts[0]
  );
}

/**
 * Extracts the curated (valid) history from a comprehensive history.
 *
 * @remarks
 * The model may sometimes generate invalid or empty contents(e.g., due to safety
 * filters or recitation). Extracting valid turns from the history
 * ensures that subsequent requests could be accepted by the model.
 * åŒæ—¶ä¹Ÿä¼šè¿‡æ»¤æ‰ reasoning å†…å®¹ï¼ˆæ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼‰
 */
function extractCuratedHistory(comprehensiveHistory: Content[]): Content[] {
  if (comprehensiveHistory === undefined || comprehensiveHistory.length === 0) {
    return [];
  }
  const curatedHistory: Content[] = [];
  const length = comprehensiveHistory.length;
  let i = 0;
  while (i < length) {
    if (comprehensiveHistory[i].role === MESSAGE_ROLES.USER) {
      curatedHistory.push(comprehensiveHistory[i]);
      i++;
    } else {
      const modelOutput: Content[] = [];
      let isValid = true;
      while (i < length && comprehensiveHistory[i].role === MESSAGE_ROLES.MODEL) {
        const currentContent = comprehensiveHistory[i];
        // è·³è¿‡ reasoning å†…å®¹ï¼Œä¸åŠ å…¥ç²¾é€‰å†å²
        if (!isReasoningContent(currentContent)) {
          modelOutput.push(currentContent);
          if (isValid && !isValidContent(currentContent)) {
            isValid = false;
          }
        }
        i++;
      }
      if (isValid && modelOutput.length > 0) {
        curatedHistory.push(...modelOutput);
      } else if (!isValid) {
        // Remove the last user input when model content is invalid.
        curatedHistory.pop();
      }
    }
  }
  return curatedHistory;
}

/**
 * Chat session that enables sending messages to the model with previous
 * conversation context.
 *
 * @remarks
 * The session maintains all the turns between user and model.
 */
export class GeminiChat {
  // A promise to represent the current state of the message being sent to the
  // model.
  private sendPromise: Promise<void> = Promise.resolve();

  // ä¿å­˜åˆ›å»ºæ—¶æŒ‡å®šçš„æ¨¡å‹ï¼Œé¿å…è¢«configè¦†ç›–
  private specifiedModel: string;

  constructor(
    private readonly config: Config,
    private readonly contentGenerator: ContentGenerator,
    private readonly generationConfig: GenerateContentConfig = {},
    private history: Content[] = [],
    private readonly agentContext: AgentContext = { type: 'main' }, // é»˜è®¤ä¸ºä¸»ä¼šè¯
    specifiedModel?: string // æ–°å¢ï¼šå…è®¸æŒ‡å®šç‰¹å®šæ¨¡å‹
  ) {
    validateHistory(history);
    // ä¼˜å…ˆä½¿ç”¨æŒ‡å®šæ¨¡å‹ï¼Œå¦åˆ™ä½¿ç”¨configæ¨¡å‹
    this.specifiedModel = specifiedModel || this.config.getModel();
  }

  private _getRequestTextFromContents(contents: Content[]): string {
    return JSON.stringify(contents);
  }

  setSpecifiedModel(model: string): void {
    this.specifiedModel = model;
  }

  private async _logApiRequest(
    contents: Content[],
    model: string,
    prompt_id: string,
  ): Promise<void> {
    const requestText = this._getRequestTextFromContents(contents);
    logApiRequest(
      this.config,
      new ApiRequestEvent(model, prompt_id, requestText),
    );
  }

  private async _logApiResponse(
    durationMs: number,
    prompt_id: string,
    usageMetadata?: GenerateContentResponseUsageMetadata,
    responseText?: string,
    agentContext?: AgentContext,
  ): Promise<void> {
    logApiResponse(
      this.config,
      new ApiResponseEvent(
        this.config.getModel(),
        durationMs,
        prompt_id,
        this.config.getContentGeneratorConfig()?.authType,
        usageMetadata,
        responseText,
        undefined, // error
        agentContext,
      ),
    );

    // Update session token statistics
    if (usageMetadata && this.config.getProjectRoot()) {
      try {
        const sessionManager = new SessionManager(this.config.getProjectRoot());
        await sessionManager.updateTokenStats(
          this.config.getSessionId(),
          this.config.getModel(),
          {
            input_token_count: usageMetadata.promptTokenCount || 0,
            output_token_count: usageMetadata.candidatesTokenCount || 0,
            total_token_count: usageMetadata.totalTokenCount || 0,
            cached_content_token_count: usageMetadata.cachedContentTokenCount || 0,
            thoughts_token_count: 0, // Not available in usageMetadata
            tool_token_count: 0, // Not available in usageMetadata
            cache_creation_input_tokens: (usageMetadata as any).cacheCreationInputTokens || 0,
            cache_read_input_tokens: (usageMetadata as any).cacheReadInputTokens || 0,
          }
        );
      } catch (error) {
        // Log error but don't fail the API response logging
        console.warn('[SessionManager] Failed to update token stats:', error);
      }

      // è§¦å‘tokenä½¿ç”¨æ›´æ–°äº‹ä»¶ï¼Œé€šçŸ¥UIæ›´æ–°
      tokenUsageEventManager.emitTokenUsage({
        cache_creation_input_tokens: (usageMetadata as any).cacheCreationInputTokens || 0,
        cache_read_input_tokens: (usageMetadata as any).cacheReadInputTokens || 0,
        input_tokens: usageMetadata.promptTokenCount || 0,
        output_tokens: usageMetadata.candidatesTokenCount || 0,
        credits_usage: (usageMetadata as any).creditsUsage || 0,
        model: this.config.getModel(),
        timestamp: Date.now(),
      });

      // æ¸…é™¤å®æ—¶tokenæ˜¾ç¤ºï¼Œå› ä¸ºè¯·æ±‚å·²å®Œæˆ
      realTimeTokenEventManager.clearRealTimeToken();
    }
  }

  private _logApiError(
    durationMs: number,
    error: unknown,
    prompt_id: string,
    agentContext?: AgentContext,
  ): void {
    const errorMessage = error instanceof Error ? error.message : String(error);
    const errorType = error instanceof Error ? error.name : 'unknown';

    logApiError(
      this.config,
      new ApiErrorEvent(
        this.config.getModel(),
        errorMessage,
        durationMs,
        prompt_id,
        this.config.getContentGeneratorConfig()?.authType,
        errorType,
        undefined, // status_code
        agentContext,
      ),
    );
  }

  /**
   * Handles falling back to Flash model when persistent 429 errors occur for OAuth users.
   * Uses a fallback handler if provided by the config; otherwise, returns null.
   */
  private async handleFlashFallback(
    authType?: string,
    error?: unknown,
  ): Promise<string | null> {
    // Only handle fallback for OAuth users
    // Flash fallback only supported for Google OAuth, not available with Cheeth OA
    return null;
  }

  /**
   * Sends a message to the model and returns the response.
   *
   * @remarks
   * This method will wait for the previous message to be processed before
   * sending the next message.
   *
   * @see {@link Chat#sendMessageStream} for streaming method.
   * @param params - parameters for sending messages within a chat session.
   * @returns The model's response.
   *
   * @example
   * ```ts
   * const chat = ai.chats.create({model: 'gemini-2.0-flash'});
   * const response = await chat.sendMessage({
   *   message: 'Why is the sky blue?'
   * });
   * console.log(response.text);
   * ```
   */
  async sendMessage(
    params: SendMessageParameters,
    prompt_id: string,
    scene: SceneType,
  ): Promise<GenerateContentResponse> {
    await this.sendPromise;
    const baseUserContent = createUserContent(params.message);
    // ğŸ¯ æ·»åŠ  prompt_id åˆ°ç”¨æˆ·å†…å®¹ä¸­
    const userContent: Content = {
      ...baseUserContent,
      prompt_id
    };
    const originalContents = this.getHistory(true).concat(userContent);

    // ğŸ”§ ä¿®æ­£è¯·æ±‚å†…å®¹ï¼Œç¡®ä¿ function call/response æˆå¯¹å‡ºç°
    const requestContents = this.fixRequestContents(originalContents);

    this._logApiRequest(requestContents, this.config.getModel(), prompt_id);

    const startTime = Date.now();
    let response: GenerateContentResponse;

    try {
      const apiCall = () => {
        const modelToUse = this.specifiedModel || DEFAULT_GEMINI_MODEL;

        // Prevent Flash model calls immediately after quota error
        if (
          this.config.getQuotaErrorOccurred() &&
          modelToUse === DEFAULT_GEMINI_FLASH_MODEL
        ) {
          throw new Error(
            'Please submit a new query to continue with the Flash model.',
          );
        }

        return this.contentGenerator.generateContent({
          model: modelToUse,
          contents: stripUIFieldsFromArray(requestContents),
          config: { ...this.generationConfig, ...params.config },
        }, scene);
      };

      response = await retryWithBackoff(apiCall, {
        shouldRetry: (error: Error) => {
          if (error && error.message) {
            if (error.message.includes('429')) return true;
            if (error.message.match(/5\d{2}/)) return true;
          }
          return false;
        },
        onPersistent429: async (authType?: string, error?: unknown) =>
          await this.handleFlashFallback(authType, error),
        authType: this.config.getContentGeneratorConfig()?.authType,
      });
      const durationMs = Date.now() - startTime;
      await this._logApiResponse(
        durationMs,
        prompt_id,
        response.usageMetadata,
        JSON.stringify(response),
        this.agentContext,
      );

      this.sendPromise = (async () => {
        const outputContent = response.candidates?.[0]?.content;
        // Because the AFC input contains the entire curated chat history in
        // addition to the new user input, we need to truncate the AFC history
        // to deduplicate the existing chat history.
        const fullAutomaticFunctionCallingHistory =
          response.automaticFunctionCallingHistory;
        const index = this.getHistory(true).length;
        let automaticFunctionCallingHistory: Content[] = [];
        if (fullAutomaticFunctionCallingHistory != null) {
          automaticFunctionCallingHistory =
            fullAutomaticFunctionCallingHistory.slice(index) ?? [];
        }
        const modelOutput = outputContent ? [outputContent] : [];
        this.recordHistory(
          userContent,
          modelOutput,
          automaticFunctionCallingHistory,
        );
      })();
      await this.sendPromise.catch(() => {
        // Resets sendPromise to avoid subsequent calls failing
        this.sendPromise = Promise.resolve();
      });
      return response;
    } catch (error) {
      const durationMs = Date.now() - startTime;
      this._logApiError(durationMs, error, prompt_id, this.agentContext);
      // æ¸…é™¤å®æ—¶tokenæ˜¾ç¤ºï¼Œå› ä¸ºè¯·æ±‚å¤±è´¥
      realTimeTokenEventManager.clearRealTimeToken();
      this.sendPromise = Promise.resolve();
      throw error;
    }
  }

  /**
   * ä¿®æ­£è¯·æ±‚å†…å®¹ï¼Œç¡®ä¿ function call å’Œ function response æˆå¯¹å‡ºç°
   *
   * å¤„ç†é€»è¾‘ï¼š
   * 1. æ£€æŸ¥å†å²ä¸­æœªå®Œæˆçš„ function call
   * 2. ä¸ºæœªå®Œæˆçš„ function call æ·»åŠ  "user cancel" response
   * 3. å¦‚æœç”¨æˆ·æ¶ˆæ¯åŒ…å«æ··åˆå†…å®¹ï¼ˆtext + function-responseï¼‰ï¼Œè°ƒæ•´é¡ºåºä¸º function-response åœ¨å‰
   * 4. ğŸ†• æ£€æµ‹å¹¶ç§»é™¤é‡å¤çš„ function responseï¼ˆåŒä¸€ä¸ª functionCall å¯¹åº”å¤šä¸ª response æ—¶å–ç¬¬ä¸€ä¸ªï¼‰
   * 5. ğŸ†• æ£€æµ‹å¹¶è­¦å‘Šå¤šä½™çš„æ— åŒ¹é… function responseï¼ˆä¿ç•™åŸæœ‰è¡Œä¸ºï¼‰
   *
   * @param requestContents åŸå§‹è¯·æ±‚å†…å®¹
   * @returns ä¿®æ­£åçš„è¯·æ±‚å†…å®¹
   */
  private fixRequestContents(requestContents: Content[]): Content[] {
    const fixedContents: Content[] = [];

    // ğŸ” è¾…åŠ©å‡½æ•°ï¼šåˆ¤æ–­ functionCall å’Œ functionResponse æ˜¯å¦åŒ¹é…
    // æ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼šå¦‚æœå…¶ä¸­ä¸€æ–¹ç¼ºå°‘ IDï¼Œåªè¦åç§°ç›¸åŒå³è§†ä¸ºåŒ¹é…ï¼ˆå…¼å®¹ Claude ç­‰æ¨¡å‹ï¼‰
    const isToolMatch = (call: any, resp: any) => {
      if (!call || !resp || call.name !== resp.name) return false;
      if (call.id && resp.id) return call.id === resp.id;
      return true; // å…¶ä¸­ä¸€æ–¹ç¼ºå°‘ IDï¼Œä»…é€šè¿‡åç§°åŒ¹é…
    };

    // ğŸ” é¢„å…ˆæ”¶é›†æ‰€æœ‰ function call ç”¨äºå¤šä½™ response æ£€æµ‹
    const allFunctionCalls: Array<{
      call: any;
      messageIndex: number;
    }> = [];

    for (let i = 0; i < requestContents.length; i++) {
      const current = requestContents[i];
      if (current.role === MESSAGE_ROLES.MODEL && current.parts) {
        current.parts.forEach(part => {
          if (part.functionCall) {
            allFunctionCalls.push({
              call: part.functionCall,
              messageIndex: i
            });
          }
        });
      }
    }

    // ğŸ¯ ç¬¬ä¸€æ­¥ï¼šæ”¶é›†å¹¶ä»²è£æ‰€æœ‰ functionResponseï¼ˆå…³é”®ä¿®å¤ï¼‰
    // å½“å­˜åœ¨å¤šä¸ªå“åº”å¯¹åº”åŒä¸€ä¸ª functionCall æ—¶ï¼ˆå¦‚ï¼šè‡ªåŠ¨è¡¥å…¨çš„ cancel vs å»¶è¿Ÿåˆ°è¾¾çš„çœŸå®ç»“æœï¼‰ï¼Œ
    // æˆ‘ä»¬æ ¹æ®ä¼˜å…ˆçº§è¿›è¡Œä»²è£ï¼šçœŸå®ç»“æœ > å–æ¶ˆå ä½ç¬¦ã€‚
    // æ³¨æ„ï¼šMap çš„ key é€»è¾‘å·²ä¼˜åŒ–ï¼Œå¦‚æœå­˜åœ¨å¸¦ ID çš„å“åº”ï¼Œå®ƒå°†è¦†ç›–åŒåä½†ä¸å¸¦ ID çš„å“åº”ã€‚
    const bestResponses: Map<string, { part: Part; priority: number; originalIndex: number }> = new Map();

    // ä¼˜å…ˆçº§åˆ¤å®šå‡½æ•°
    const getPriority = (part: Part): number => {
      const result = (part.functionResponse?.response as any)?.result;
      return result === 'user cancel' ? 10 : 100;
    };

    // 1.1 é¢„æ‰«ææ‰€æœ‰æ¶ˆæ¯ï¼Œæ‰¾å‡ºæ¯ä¸ª callId çš„æœ€ä½³å“åº”
    for (let i = 0; i < requestContents.length; i++) {
      const content = requestContents[i];
      if (content.role === MESSAGE_ROLES.USER && content.parts) {
        for (const part of content.parts) {
          if (part.functionResponse) {
            const resp = part.functionResponse;
            const priority = getPriority(part);

            // æ™ºèƒ½ Keyï¼šå¦‚æœå¸¦ IDï¼Œä¼˜å…ˆä½¿ç”¨ IDï¼›å¦åˆ™ä½¿ç”¨ name
            // è¿™æ ·å¸¦ ID çš„çœŸå®ç»“æœå¯ä»¥è¦†ç›–ä¸å¸¦ ID çš„å ä½ç¬¦ï¼ˆClaude åœºæ™¯ï¼‰
            const key = resp.id || `name:${resp.name}`;

            const existing = bestResponses.get(key);
            if (!existing || priority > existing.priority) {
              bestResponses.set(key, { part, priority, originalIndex: i });
            }

            // ç‰¹æ®Šé€»è¾‘ï¼šå¦‚æœæ˜¯å¸¦ ID çš„å“åº”ï¼Œè¿˜è¦å°è¯•è¦†ç›–æ‰åªæœ‰ name çš„è®°å½•
            if (resp.id) {
              const nameKey = `name:${resp.name}`;
              const nameExisting = bestResponses.get(nameKey);
              if (nameExisting && priority >= nameExisting.priority) {
                bestResponses.delete(nameKey); // è®©ä½ç»™å¸¦ç²¾å‡† ID çš„å“åº”
              }
            }
          }
        }
      }
    }

    // 1.2 é‡æ„å†…å®¹ï¼Œåªä¿ç•™æœ€ä½³å“åº”ï¼Œå¹¶å¼ºåˆ¶ ID å¯¹é½
    const deduplicatedContents: Content[] = [];
    const usedResponseKeys: Set<string> = new Set();

    for (let i = 0; i < requestContents.length; i++) {
      const content = requestContents[i];
      if (content.role === MESSAGE_ROLES.USER && content.parts) {
        const filteredParts: Part[] = [];

        for (const part of content.parts) {
          if (part.functionResponse) {
            const resp = part.functionResponse;
            const key = resp.id || `name:${resp.name}`;
            const best = bestResponses.get(key);

            // åªæœ‰å½“å½“å‰ Part å°±æ˜¯è¯¥ callId çš„â€œæœ€ä½³å“åº”â€æ—¶ï¼Œæ‰ä¿ç•™å®ƒ
            if (best && best.part === part && !usedResponseKeys.has(key)) {
              // ğŸ¯ å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ ID å¯¹é½
              // æŸ¥æ‰¾è¯¥å“åº”å¯¹åº”çš„åŸå§‹ functionCallï¼Œç¡®ä¿ ID å®Œå…¨ä¸€è‡´ï¼ˆå…¼å®¹ Claude ä¸¥æ ¼åè®®ï¼‰
              const matchingCall = allFunctionCalls.find(fc => isToolMatch(fc.call, resp));
              if (matchingCall) {
                if (matchingCall.call.id !== resp.id) {
                  console.log(
                    `[fixRequestContents] ğŸ”§ ID å¯¹é½ï¼šå°†å“åº” ${resp.name} çš„ ID ä» "${resp.id || 'unnamed'}" ` +
                    `åŒæ­¥ä¸ºè°ƒç”¨æ–¹çš„ ID "${matchingCall.call.id || 'unnamed'}"`
                  );
                  resp.id = matchingCall.call.id;
                }
              }

              filteredParts.push(part);
              usedResponseKeys.add(key);
            } else {
              // å¦‚æœä¸å¸¦ ID çš„å“åº”è¢«å¸¦ ID çš„å“åº”å–ä»£äº†ï¼Œä¹Ÿä¼šè¿›å…¥è¿™é‡Œ
              console.warn(
                `[fixRequestContents] ğŸ—‘ï¸ ç§»é™¤æ¬¡ä¼˜æˆ–é‡å¤çš„ functionResponseï¼š${resp.name} (id: ${resp.id || 'unnamed'})ã€‚` +
                `ä¿ç•™ä¼˜å…ˆçº§æ›´é«˜æˆ–æ›´ç²¾å‡†çš„å“åº”ã€‚`
              );
            }
          } else {
            filteredParts.push(part);
          }
        }

        if (filteredParts.length > 0) {
          deduplicatedContents.push({ ...content, parts: filteredParts });
        }
      } else {
        deduplicatedContents.push(content);
      }
    }

    for (let i = 0; i < deduplicatedContents.length; i++) {
      const current = deduplicatedContents[i];
      fixedContents.push(current);

      // ğŸ†• æ£€æµ‹ç”¨æˆ·æ¶ˆæ¯ä¸­çš„å­¤ç«‹ function responseï¼ˆæ— åŒ¹é…çš„ functionCallï¼‰
      if (current.role === MESSAGE_ROLES.USER && current.parts) {
        const functionResponses = current.parts.filter(part => part.functionResponse);
        if (functionResponses.length > 0) {
          const orphanedResponses = functionResponses.filter(respPart => {
            const functionResponse = respPart.functionResponse!;
            return !allFunctionCalls.some(({ call }) => {
              // ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…é€»è¾‘
              return isToolMatch(call, functionResponse);
            });
          });

          if (orphanedResponses.length > 0) {
            console.log(
              `[fixRequestContents] æ£€æµ‹åˆ°ç¬¬${i + 1}æ¡æ¶ˆæ¯ä¸­æœ‰ ${orphanedResponses.length} ä¸ªå­¤ç«‹çš„ function response:`,
              orphanedResponses.map(r => ({
                name: r.functionResponse!.name,
                id: r.functionResponse!.id,
                result: (r.functionResponse!.response as any)?.result
              }))
            );
          }
        }
      }

      // æ£€æŸ¥å½“å‰æ¶ˆæ¯æ˜¯å¦åŒ…å« function call
      const hasFunctionCall = current.role === MESSAGE_ROLES.MODEL &&
        current.parts?.some(part => part.functionCall);

      if (hasFunctionCall) {
        const next = deduplicatedContents[i + 1];

        // è·å–å½“å‰æ¶ˆæ¯ä¸­çš„æ‰€æœ‰ function call
        const functionCalls = current.parts?.filter(part => part.functionCall) || [];

        if (functionCalls.length > 0) {
          // æ£€æŸ¥ä¸‹ä¸€æ¡æ¶ˆæ¯ä¸­çš„ function response
          const nextFunctionResponses = next?.role === MESSAGE_ROLES.USER && next.parts ?
            next.parts.filter(part => part.functionResponse) : [];

          // æ‰¾å‡ºæœªåŒ¹é…çš„ function callï¼ˆä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ï¼‰
          const unmatchedCalls = functionCalls.filter(callPart => {
            const functionCall = callPart.functionCall!;
            return !nextFunctionResponses.some(respPart => {
              const functionResponse = respPart.functionResponse!;
              return isToolMatch(functionCall, functionResponse);
            });
          });

          // ä¸ºæœªåŒ¹é…çš„ function call åˆ›å»º "user cancel" response
          if (unmatchedCalls.length > 0) {
            const cancelResponses = unmatchedCalls.map(part => {
              const functionCall = part.functionCall!;
              return {
                functionResponse: {
                  name: functionCall.name,
                  response: { result: 'user cancel' },
                  ...(functionCall.id && { id: functionCall.id })
                }
              };
            });

            // æ’å…¥è¡¥å…¨çš„ function response
            fixedContents.push({
              role: MESSAGE_ROLES.USER,
              parts: cancelResponses
            });

            console.log(`[fixRequestContents] ä¸ºç¬¬${i + 1}æ¡æ¶ˆæ¯è¡¥å…¨äº† ${unmatchedCalls.length} ä¸ªæœªåŒ¹é…çš„ function call`);
          }

          // å¦‚æœä¸‹ä¸€æ¡æ¶ˆæ¯æœ‰æ··åˆå†…å®¹ï¼Œè°ƒæ•´ parts é¡ºåºï¼šfunction-response åœ¨å‰ï¼Œtext åœ¨å
          if (next && nextFunctionResponses.length > 0) {
            const textParts = next.parts?.filter(part => !part.functionResponse) || [];

            if (textParts.length > 0) {
              // ä¿®æ”¹ä¸‹ä¸€æ¡æ¶ˆæ¯çš„ parts é¡ºåº
              deduplicatedContents[i + 1] = {
                ...next,
                parts: [...nextFunctionResponses, ...textParts]
              };
              console.log(`[fixRequestContents] è°ƒæ•´äº†ç¬¬${i + 2}æ¡æ¶ˆæ¯çš„å†…å®¹é¡ºåºï¼Œfunction-response åœ¨å‰`);
            }
          }
        }
      }
    }

    // ğŸ†• æœ€ç»ˆæ¸…ç†ï¼šç§»é™¤æ‰€æœ‰ä»ç„¶å­¤ç«‹çš„ functionResponse
    // è¿™å¤„ç†äº† "functionResponse without preceding functionCall" çš„æƒ…å†µ
    // è¿™ç§æƒ…å†µå¯èƒ½å‘ç”Ÿåœ¨å‹ç¼©åï¼Œæˆ–è€…å†å²è®°å½•æŸåæ—¶
    const finalContents: Content[] = [];
    const finalToolCallStack: { [id: string]: boolean } = {};
    const finalToolCallNames: { [name: string]: boolean } = {};

    for (const content of fixedContents) {
      // è®°å½•æ‰€æœ‰ function call
      if (content.role === MESSAGE_ROLES.MODEL && content.parts) {
        content.parts.forEach(part => {
          if (part.functionCall) {
            if (part.functionCall.id) finalToolCallStack[part.functionCall.id] = true;
            if (part.functionCall.name) finalToolCallNames[part.functionCall.name] = true;
          }
        });
        finalContents.push(content);
      } else if (content.role === MESSAGE_ROLES.USER && content.parts) {
        // è¿‡æ»¤ functionResponse
        const validParts = content.parts.filter(part => {
          if (!part.functionResponse) return true; // ä¿ç•™é functionResponse éƒ¨åˆ†

          const response = part.functionResponse;
          const hasMatchingId = response.id && finalToolCallStack[response.id];
          const hasMatchingName = response.name && finalToolCallNames[response.name];

          if (hasMatchingId || hasMatchingName) {
            return true;
          } else {
            console.warn(
              `[fixRequestContents] âŒ ç§»é™¤å­¤ç«‹çš„ functionResponseï¼š${response.name} (id: ${response.id})ã€‚` +
              `è¿™ä¸ª response æ²¡æœ‰å¯¹åº”çš„ function callã€‚`
            );
            return false;
          }
        });

        if (validParts.length > 0) {
          finalContents.push({ ...content, parts: validParts });
        } else {
          console.warn(`[fixRequestContents] ç§»é™¤ç©ºçš„ç”¨æˆ·æ¶ˆæ¯ï¼ˆæ‰€æœ‰ functionResponse éƒ½è¢«è¿‡æ»¤ï¼‰`);
        }
      } else {
        finalContents.push(content);
      }
    }

    return finalContents;
  }

  /**
   * Sends a message to the model and returns the response in chunks.
   *
   * @remarks
   * This method will wait for the previous message to be processed before
   * sending the next message.
   *
   * @see {@link Chat#sendMessage} for non-streaming method.
   * @param params - parameters for sending the message.
   * @return The model's response.
   *
   * @example
   * ```ts
   * const chat = ai.chats.create({model: 'gemini-2.0-flash'});
   * const response = await chat.sendMessageStream({
   *   message: 'Why is the sky blue?'
   * });
   * for await (const chunk of response) {
   *   console.log(chunk.text);
   * }
   * ```
   */
  async sendMessageStream(
    params: SendMessageParameters,
    prompt_id: string,
    scene: SceneType,
  ): Promise<AsyncGenerator<GenerateContentResponse>> {
    await this.sendPromise;

    const baseUserContent = createUserContent(params.message);
    // ğŸ¯ æ·»åŠ  prompt_id åˆ°ç”¨æˆ·å†…å®¹ä¸­
    const userContent: Content = {
      ...baseUserContent,
      prompt_id
    };
    const originalContents = this.getHistory(true).concat(userContent);

    // ğŸ”§ ä¿®æ­£è¯·æ±‚å†…å®¹ï¼Œç¡®ä¿ function call/response æˆå¯¹å‡ºç°
    const requestContents = this.fixRequestContents(originalContents);
    this._logApiRequest(requestContents, this.config.getModel(), prompt_id);

    const startTime = Date.now();

    try {
      const apiCall = () => {
        const modelToUse = this.specifiedModel || DEFAULT_GEMINI_MODEL;

        // Prevent Flash model calls immediately after quota error
        if (
          this.config.getQuotaErrorOccurred() &&
          modelToUse === DEFAULT_GEMINI_FLASH_MODEL
        ) {
          throw new Error(
            'Please submit a new query to continue with the Flash model.',
          );
        }

        return this.contentGenerator.generateContentStream({
          model: modelToUse,
          contents: stripUIFieldsFromArray(requestContents),
          config: { ...this.generationConfig, ...params.config },
        }, scene);
      };

      // Note: Retrying streams can be complex. If generateContentStream itself doesn't handle retries
      // for transient issues internally before yielding the async generator, this retry will re-initiate
      // the stream. For simple 429/500 errors on initial call, this is fine.
      // If errors occur mid-stream, this setup won't resume the stream; it will restart it.
      const streamResponse = await retryWithBackoff(apiCall, {
        shouldRetry: (error: Error) => {
          // ğŸš« DeepXé…é¢é”™è¯¯ä¸åº”é‡è¯• - éœ€è¦ç«‹å³æ˜¾ç¤ºå‹å¥½æç¤º
          if (isDeepXQuotaError(error)) {
            return false;
          }

          // Check error messages for status codes, or specific error names if known
          if (error && error.message) {
            if (error.message.includes('429')) return true;
            // 451é”™è¯¯ä¸é‡è¯• - ç«‹å³å¤±è´¥
            if (error.message.includes('REGION_BLOCKED_451') || error.message.includes('451')) return false;
            if (error.message.match(/5\d{2}/)) return true;
          }
          return false; // Don't retry other errors by default
        },
        onPersistent429: async (authType?: string, error?: unknown) =>
          await this.handleFlashFallback(authType, error),
        authType: this.config.getContentGeneratorConfig()?.authType,
      });

      // Resolve the internal tracking of send completion promise - `sendPromise`
      // for both success and failure response. The actual failure is still
      // propagated by the `await streamResponse`.
      this.sendPromise = Promise.resolve(streamResponse)
        .then(() => undefined)
        .catch(() => undefined);

      const result = this.processStreamResponse(
        streamResponse,
        userContent,
        startTime,
        prompt_id,
      );
      return result;
    } catch (error) {
      const durationMs = Date.now() - startTime;
      this._logApiError(durationMs, error, prompt_id, this.agentContext);
      // æ¸…é™¤å®æ—¶tokenæ˜¾ç¤ºï¼Œå› ä¸ºè¯·æ±‚å¤±è´¥
      realTimeTokenEventManager.clearRealTimeToken();
      this.sendPromise = Promise.resolve();
      throw error;
    }
  }

  /**
   * Returns the chat history.
   *
   * @remarks
   * The history is a list of contents alternating between user and model.
   *
   * There are two types of history:
   * - The `curated history` contains only the valid turns between user and
   * model, which will be included in the subsequent requests sent to the model.
   * - The `comprehensive history` contains all turns, including invalid or
   *   empty model outputs, providing a complete record of the history.
   *
   * The history is updated after receiving the response from the model,
   * for streaming response, it means receiving the last chunk of the response.
   *
   * The `comprehensive history` is returned by default. To get the `curated
   * history`, set the `curated` parameter to `true`.
   *
   * @param curated - whether to return the curated history or the comprehensive
   *     history.
   * @return History contents alternating between user and model for the entire
   *     chat session.
   */
  getHistory(curated: boolean = false): Content[] {
    const history = curated
      ? extractCuratedHistory(this.history)
      : this.history;
    // Deep copy the history to avoid mutating the history outside of the
    // chat session.
    return structuredClone(history);
  }

  /**
   * Clears the chat history.
   */
  clearHistory(): void {
    this.history = [];
  }

  /**
   * Adds a new entry to the chat history.
   *
   * @param content - The content to add to the history.
   */
  addHistory(content: Content): void {
    this.history.push(content);
  }
  setHistory(history: Content[]): void {
    this.history = history;
  }

  setTools(tools: Tool[]): void {
    this.generationConfig.tools = tools;
  }

  /**
   * è·å–å·¥å…·å£°æ˜ï¼ˆç”¨äº token è®¡æ•°ç­‰åœºæ™¯ï¼‰
   * @returns å·¥å…·åˆ—è¡¨ï¼Œå¦‚æœæœªè®¾ç½®åˆ™è¿”å› undefined
   */
  getTools(): typeof this.generationConfig.tools {
    return this.generationConfig.tools;
  }

  /**
   * è·å–ç³»ç»ŸæŒ‡ä»¤ï¼ˆç”¨äº token è®¡æ•°ç­‰åœºæ™¯ï¼‰
   * @returns ç³»ç»ŸæŒ‡ä»¤å†…å®¹ï¼Œå¦‚æœæœªè®¾ç½®åˆ™è¿”å› undefined
   */
  getSystemInstruction(): ContentUnion | undefined {
    return this.generationConfig.systemInstruction;
  }

  getFinalUsageMetadata(
    chunks: GenerateContentResponse[],
  ): GenerateContentResponseUsageMetadata | undefined {
    const lastChunkWithMetadata = chunks
      .slice()
      .reverse()
      .find((chunk) => chunk.usageMetadata);

    return lastChunkWithMetadata?.usageMetadata;
  }

  private async *processStreamResponse(
    streamResponse: AsyncGenerator<GenerateContentResponse>,
    inputContent: Content,
    startTime: number,
    prompt_id: string,
  ) {
    const outputContent: Content[] = [];
    const chunks: GenerateContentResponse[] = [];
    let errorOccurred = false;

    try {
      for await (const chunk of streamResponse) {
        // å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯ reasoning å†…å®¹ï¼Œå¦‚æœæ˜¯å°±è·³è¿‡ä¸åŠ å…¥ chunks
        const content = chunk.candidates?.[0]?.content;
        const isReasoning = content && this.isReasoningContent(content);
        const isThought = content && this.isThoughtContent(content);

        // æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„å—ï¼Œä½†æ’é™¤ thought å’Œ reasoning
        if ((isValidResponse(chunk) || chunk.usageMetadata) && !isReasoning && !isThought) {
          chunks.push(chunk);
        }

        // å¤„ç†åŒ…å«å†…å®¹çš„æœ‰æ•ˆå“åº”
        if (isValidResponse(chunk)) {
          if (content !== undefined) {
            // è·³è¿‡ thought å’Œ reasoning å†…å®¹ï¼Œä¸åŠ å…¥å†å²è®°å½•
            if (isThought || isReasoning) {
              yield chunk;
              continue;
            }
            outputContent.push(content);
          }
        }
        yield chunk;
      }
    } catch (error) {
      errorOccurred = true;
      const durationMs = Date.now() - startTime;
      this._logApiError(durationMs, error, prompt_id, this.agentContext);
      // æ¸…é™¤å®æ—¶tokenæ˜¾ç¤ºï¼Œå› ä¸ºè¯·æ±‚å¤±è´¥
      realTimeTokenEventManager.clearRealTimeToken();

      // ğŸ¯ å…³é”®ä¿®å¤ï¼šå³ä½¿å‘ç”Ÿé”™è¯¯ï¼ˆå¦‚ç”¨æˆ·å–æ¶ˆï¼‰ï¼Œä¹Ÿè¦è®°å½•å·²ç»æ”¶åˆ°çš„å†…å®¹
      // å¦‚æœæ¨¡å‹å·²ç»è¾“å‡ºäº†å†…å®¹ï¼ˆå°¤å…¶æ˜¯ functionCallï¼‰ï¼Œè®°å½•å®ƒèƒ½ä¿æŒå†å²è®°å½•çš„å®Œæ•´æ€§ï¼Œ
      // é¿å…åç»­å·¥å…·æ‰§è¡Œç»“æœå˜æˆâ€œå­¤ç«‹å“åº”â€ã€‚
      if (outputContent.length > 0) {
        this.recordHistory(inputContent, outputContent);
      }

      throw error;
    }

    if (!errorOccurred) {
      const durationMs = Date.now() - startTime;
      const allParts: Part[] = [];
      for (const content of outputContent) {
        if (content.parts) {
          allParts.push(...content.parts);
        }
      }
      await this._logApiResponse(
        durationMs,
        prompt_id,
        this.getFinalUsageMetadata(chunks),
        JSON.stringify(chunks),
        this.agentContext,
      );
      // ğŸ¯ æ­£å¸¸ç»“æŸæ—¶è®°å½•å†å²
      this.recordHistory(inputContent, outputContent);
    }
  }

  private recordHistory(
    userInput: Content,
    modelOutput: Content[],
    automaticFunctionCallingHistory?: Content[],
  ) {
    // è¿‡æ»¤æ‰ thought å’Œ reasoning å†…å®¹
    const nonThoughtModelOutput = modelOutput.filter(
      (content) => !this.isThoughtContent(content) && !this.isReasoningContent(content),
    );

    let outputContents: Content[] = [];
    if (
      nonThoughtModelOutput.length > 0 &&
      nonThoughtModelOutput.every((content) => content.role !== undefined)
    ) {
      outputContents = nonThoughtModelOutput;
    } else if (nonThoughtModelOutput.length === 0 && modelOutput.length > 0) {
      // This case handles when the model returns only a thought.
      // We don't want to add an empty model response in this case.
    } else {
      // When not a function response appends an empty content when model returns empty response, so that the
      // history is always alternating between user and model.
      // Workaround for: https://b.corp.google.com/issues/420354090
      if (!isFunctionResponse(userInput)) {
        outputContents.push({
          role: MESSAGE_ROLES.MODEL,
          parts: [],
        } as Content);
      }
    }
    if (
      automaticFunctionCallingHistory &&
      automaticFunctionCallingHistory.length > 0
    ) {
      this.history.push(
        ...extractCuratedHistory(automaticFunctionCallingHistory),
      );
    } else {
      this.history.push(userInput);
    }

    // ğŸ”§ Enhanced consolidation logic to merge function calls into single messages
    const consolidatedOutputContents: Content[] = [];
    for (const content of outputContents) {
      // è·³è¿‡ thought å’Œ reasoning å†…å®¹
      if (this.isThoughtContent(content) || this.isReasoningContent(content)) {
        continue;
      }
      const lastContent =
        consolidatedOutputContents[consolidatedOutputContents.length - 1];

      // Check if current content has function calls
      const hasFunctionCalls = content.parts?.some(part => part.functionCall);
      const lastHasFunctionCalls = lastContent?.parts?.some(part => part.functionCall);

      if (this.isTextContent(lastContent) && this.isTextContent(content)) {
        // If both current and last are text, combine their text into the lastContent's first part
        // and append any other parts from the current content.
        lastContent.parts[0].text += content.parts[0].text || '';
        if (content.parts.length > 1) {
          lastContent.parts.push(...content.parts.slice(1));
        }
      } else if (hasFunctionCalls && lastHasFunctionCalls && lastContent.role === MESSAGE_ROLES.MODEL) {
        // ğŸš€ KEY FIX: Merge consecutive function calls into the same message
        // This ensures multiple function calls are stored as one model message with multiple parts
        console.log('[recordHistory] Merging consecutive function calls into single message');
        lastContent.parts?.push(...(content.parts || []));
      } else {
        consolidatedOutputContents.push(content);
      }
    }

    if (consolidatedOutputContents.length > 0) {
      const lastHistoryEntry = this.history[this.history.length - 1];
      const canMergeWithLastHistory =
        !automaticFunctionCallingHistory ||
        automaticFunctionCallingHistory.length === 0;

      if (
        canMergeWithLastHistory &&
        this.isTextContent(lastHistoryEntry) &&
        this.isTextContent(consolidatedOutputContents[0])
      ) {
        // If both current and last are text, combine their text into the lastHistoryEntry's first part
        // and append any other parts from the current content.
        lastHistoryEntry.parts[0].text +=
          consolidatedOutputContents[0].parts[0].text || '';
        if (consolidatedOutputContents[0].parts.length > 1) {
          lastHistoryEntry.parts.push(
            ...consolidatedOutputContents[0].parts.slice(1),
          );
        }
        consolidatedOutputContents.shift(); // Remove the first element as it's merged
      }
      this.history.push(...consolidatedOutputContents);
    }
  }

  private isTextContent(
    content: Content | undefined,
  ): content is Content & { parts: [{ text: string }, ...Part[]] } {
    return !!(
      content &&
      content.role === 'model' &&
      content.parts &&
      content.parts.length > 0 &&
      typeof content.parts[0].text === 'string' &&
      content.parts[0].text !== ''
    );
  }

  private isThoughtContent(
    content: Content | undefined,
  ): content is Content & { parts: [{ thought: boolean }, ...Part[]] } {
    return !!(
      content &&
      content.role === 'model' &&
      content.parts &&
      content.parts.length > 0 &&
      typeof content.parts[0].thought === 'boolean' &&
      content.parts[0].thought === true
    );
  }

  /**
   * æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºæ¨¡å‹çš„ reasoningï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
   * reasoning ä¸åº”è¯¥è¢«æ·»åŠ åˆ°å†å²è®°å½•ä¸­
   * ç›´æ¥è°ƒç”¨å¤–éƒ¨å‡½æ•°
   */
  private isReasoningContent(content: Content | undefined): boolean {
    return isReasoningContent(content);
  }
}
