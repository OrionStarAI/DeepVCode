/**
 * @license
 * Copyright 2025 DeepV Code team
 * SPDX-License-Identifier: Apache-2.0
 */

import {
  GenerateContentResponse,
  FinishReason,
} from '@google/genai';
import { CustomModelConfig } from '../types/customModel.js';
import { MESSAGE_ROLES } from '../config/messageRoles.js';
import { retryWithBackoff, getErrorStatus } from '../utils/retry.js';

/**
 * ä¸ºå¯¹è±¡æ·»åŠ  functionCalls getterï¼Œå…¼å®¹ä¸åŒçš„ç»“æ„
 * - GenerateContentResponse ç»“æ„: response.candidates[0].content.parts
 * - Content ç»“æ„: content.parts
 */
function addFunctionCallsGetter(obj: any) {
  if (!obj) return;

  // æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰è¯¥å±æ€§æˆ– getter
  const descriptor = Object.getOwnPropertyDescriptor(obj, 'functionCalls');
  if (descriptor) return;

  Object.defineProperty(obj, 'functionCalls', {
    get: function() {
      // ä¼˜å…ˆå°è¯• GenerateContentResponse ç»“æ„
      const partsFromResponse = this.candidates?.[0]?.content?.parts;
      // å¦‚æœä¸æ˜¯ GenerateContentResponseï¼Œå°è¯• Content ç»“æ„
      const parts = partsFromResponse || this.parts;

      if (!parts || !Array.isArray(parts)) return undefined;

      const calls = parts
        .filter((p: any) => p && p.functionCall)
        .map((p: any) => p.functionCall);

      return calls.length > 0 ? calls : undefined;
    },
    enumerable: false,
    configurable: true
  });
}

/**
 * ç¯å¢ƒå˜é‡æ›¿æ¢å‡½æ•°
 */
function resolveEnvVar(value: string): string {
  const envVarRegex = /\$\{([^}]+)\}|\$(\w+)/g;
  return value.replace(envVarRegex, (match, varName1, varName2) => {
    const varName = varName1 || varName2;
    return process.env[varName] || match;
  });
}

/**
 * å®‰å…¨è§£æ JSON - å¢å¼ºç‰ˆ
 * ä¸“é—¨é’ˆå¯¹æµå¼å·¥å…·è°ƒç”¨åœºæ™¯ä¼˜åŒ–ï¼Œå¤„ç†å„ç§ä¸å®Œæ•´æˆ–æ ¼å¼å¼‚å¸¸çš„ JSON
 *
 * å¸¸è§é—®é¢˜åœºæ™¯ï¼š
 * 1. æµå¼ä¼ è¾“ä¸­ JSON è¢«æˆªæ–­ï¼š{"pattern": "TO  (ç¼ºå°‘ç»“å°¾)
 * 2. æ¨¡å‹è¿”å›ç©ºå­—ç¬¦ä¸²æˆ– undefined
 * 3. æ¨¡å‹è¿”å›éæ ‡å‡†æ ¼å¼ï¼ˆå¦‚å¸¦æœ‰å¤šä½™ç©ºæ ¼ã€æ¢è¡Œï¼‰
 * 4. åµŒå¥— JSON å­—ç¬¦ä¸²ï¼ˆéœ€è¦äºŒæ¬¡è§£æï¼‰
 */
function parseJSONSafe(jsonStr: string): any {
  // å¤„ç†ç©ºå€¼
  if (!jsonStr || jsonStr === 'null' || jsonStr === 'undefined') {
    return {};
  }

  // å¦‚æœå·²ç»æ˜¯å¯¹è±¡ï¼Œç›´æ¥è¿”å›
  if (typeof jsonStr === 'object') {
    return jsonStr;
  }

  // æ¸…ç†å­—ç¬¦ä¸²
  let cleanStr = jsonStr.trim();

  // å¤„ç†ç©ºå¯¹è±¡å­—ç¬¦ä¸²
  if (cleanStr === '{}' || cleanStr === '') {
    return {};
  }

  // ç¬¬ä¸€æ¬¡å°è¯•ï¼šç›´æ¥è§£æ
  try {
    return JSON.parse(cleanStr);
  } catch (firstError) {
    // ç»§ç»­å°è¯•ä¿®å¤
  }

  // ä¿®å¤ç­–ç•¥ 1ï¼šå¤„ç†ä¸å®Œæ•´çš„ JSON å¯¹è±¡
  if (cleanStr.startsWith('{') && !cleanStr.endsWith('}')) {
    const repaired = repairIncompleteJSON(cleanStr);
    if (repaired) {
      try {
        return JSON.parse(repaired);
      } catch {
        // ç»§ç»­å°è¯•å…¶ä»–æ–¹æ³•
      }
    }
  }

  // ä¿®å¤ç­–ç•¥ 2ï¼šå¤„ç†ä¸å®Œæ•´çš„ JSON æ•°ç»„
  if (cleanStr.startsWith('[') && !cleanStr.endsWith(']')) {
    // å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„å…ƒç´ 
    const lastCompleteComma = cleanStr.lastIndexOf(',');
    if (lastCompleteComma > 0) {
      const repaired = cleanStr.substring(0, lastCompleteComma) + ']';
      try {
        return JSON.parse(repaired);
      } catch {
        // ç»§ç»­å°è¯•
      }
    }
    // å°è¯•ç›´æ¥è¡¥å…¨
    try {
      return JSON.parse(cleanStr + ']');
    } catch {
      // ç»§ç»­å°è¯•
    }
  }

  // ä¿®å¤ç­–ç•¥ 3ï¼šç§»é™¤å°¾éƒ¨å¯èƒ½çš„åƒåœ¾å­—ç¬¦
  // æœ‰æ—¶æ¨¡å‹ä¼šåœ¨ JSON åé™„åŠ é¢å¤–å†…å®¹
  const jsonEndMatch = cleanStr.match(/^(\{[\s\S]*\}|\[[\s\S]*\])/);
  if (jsonEndMatch) {
    try {
      return JSON.parse(jsonEndMatch[1]);
    } catch {
      // ç»§ç»­å°è¯•
    }
  }

  // ä¿®å¤ç­–ç•¥ 4ï¼šå¤„ç†è½¬ä¹‰é—®é¢˜
  // æœ‰æ—¶ JSON å­—ç¬¦ä¸²ä¸­çš„å¼•å·æ²¡æœ‰æ­£ç¡®è½¬ä¹‰
  try {
    // å°è¯•ä¿®å¤å¸¸è§çš„è½¬ä¹‰é—®é¢˜
    const fixedEscape = cleanStr
      .replace(/([^\\])\\([^"\\/bfnrtu])/g, '$1\\\\$2')  // ä¿®å¤æ— æ•ˆè½¬ä¹‰
      .replace(/\t/g, '\\t')  // æ›¿æ¢å®é™…çš„ tab
      .replace(/\n/g, '\\n')  // æ›¿æ¢å®é™…çš„æ¢è¡Œ
      .replace(/\r/g, '\\r'); // æ›¿æ¢å®é™…çš„å›è½¦
    return JSON.parse(fixedEscape);
  } catch {
    // ç»§ç»­å°è¯•
  }

  // æ‰€æœ‰ä¿®å¤å°è¯•éƒ½å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶è¿”å›å¸¦æ ‡è®°çš„å¯¹è±¡
  console.error(`[CustomModel] Failed to parse tool arguments after all repair attempts`);
  console.error(`[CustomModel] Original string (first 500 chars): ${jsonStr.substring(0, 500)}`);

  // è¿”å›ä¸€ä¸ªæ ‡è®°äº†è§£æé”™è¯¯çš„å¯¹è±¡
  // ä½¿ç”¨ __parseError å‰ç¼€é¿å…ä¸æ­£å¸¸å·¥å…·å‚æ•°å†²çª
  return {
    __parseError: true,
    __rawArgs: jsonStr,
    __errorMessage: `Failed to parse tool arguments as JSON. Raw value: ${jsonStr.substring(0, 200)}${jsonStr.length > 200 ? '...' : ''}`
  };
}

/**
 * å°è¯•ä¿®å¤ä¸å®Œæ•´çš„ JSON å¯¹è±¡
 * ä½¿ç”¨æ‹¬å·åŒ¹é…å’Œå¼•å·çŠ¶æ€è¿½è¸ªæ¥æ‰¾åˆ°å¯ä»¥å®‰å…¨æˆªæ–­çš„ä½ç½®
 */
function repairIncompleteJSON(jsonStr: string): string | null {
  let braceCount = 0;
  let bracketCount = 0;
  let inString = false;
  let escapeNext = false;
  let lastSafePosition = -1;
  let lastKeyValueEnd = -1;

  for (let i = 0; i < jsonStr.length; i++) {
    const char = jsonStr[i];

    if (escapeNext) {
      escapeNext = false;
      continue;
    }

    if (char === '\\') {
      escapeNext = true;
      continue;
    }

    if (char === '"' && !escapeNext) {
      inString = !inString;
      continue;
    }

    if (inString) continue;

    switch (char) {
      case '{':
        braceCount++;
        break;
      case '}':
        braceCount--;
        if (braceCount === 0) {
          lastSafePosition = i;
        }
        break;
      case '[':
        bracketCount++;
        break;
      case ']':
        bracketCount--;
        break;
      case ',':
        // é€—å·åé¢å¯èƒ½æ˜¯å®‰å…¨çš„æˆªæ–­ç‚¹ï¼ˆå¦‚æœä¸åœ¨åµŒå¥—ç»“æ„ä¸­ï¼‰
        if (braceCount === 1 && bracketCount === 0) {
          lastKeyValueEnd = i;
        }
        break;
    }
  }

  // å¦‚æœæ‰¾åˆ°äº†å®Œæ•´çš„ JSONï¼Œç›´æ¥è¿”å›
  if (lastSafePosition > 0 && braceCount === 0) {
    return jsonStr.substring(0, lastSafePosition + 1);
  }

  // å°è¯•åœ¨æœ€åä¸€ä¸ªé€—å·å¤„æˆªæ–­å¹¶è¡¥å…¨
  if (lastKeyValueEnd > 0) {
    const truncated = jsonStr.substring(0, lastKeyValueEnd);
    // è¡¥å…¨ç¼ºå¤±çš„æ‹¬å·
    let result = truncated;
    for (let i = 0; i < braceCount; i++) {
      result += '}';
    }
    for (let i = 0; i < bracketCount; i++) {
      result += ']';
    }
    return result;
  }

  // å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„é”®å€¼å¯¹ï¼ˆä»¥ " ç»“å°¾çš„å€¼ï¼‰
  // ä¾‹å¦‚: {"pattern": "TODO", "path": "/src  -> æˆªæ–­åˆ° "TODO"
  const patterns = [
    /^(.*"[^"]*"\s*:\s*"[^"]*")\s*,?\s*"[^"]*"\s*:\s*"?[^"}]*$/,  // æˆªæ–­åˆ°ä¸Šä¸€ä¸ªå®Œæ•´çš„å­—ç¬¦ä¸²å€¼
    /^(.*"[^"]*"\s*:\s*\d+)\s*,?\s*"[^"]*"\s*:\s*"?[^"}]*$/,       // æˆªæ–­åˆ°ä¸Šä¸€ä¸ªå®Œæ•´çš„æ•°å­—å€¼
    /^(.*"[^"]*"\s*:\s*(?:true|false|null))\s*,?\s*"[^"]*"\s*:\s*"?[^"}]*$/,  // æˆªæ–­åˆ°å¸ƒå°”/nullå€¼
  ];

  for (const pattern of patterns) {
    const match = jsonStr.match(pattern);
    if (match && match[1]) {
      return match[1] + '}';
    }
  }

  // æœ€åçš„å°è¯•ï¼šç›´æ¥è¡¥å…¨æ‹¬å·
  if (braceCount > 0) {
    let result = jsonStr;
    // å¦‚æœåœ¨å­—ç¬¦ä¸²ä¸­é—´è¢«æˆªæ–­ï¼Œå…ˆè¡¥å…¨å¼•å·
    if (inString) {
      result += '"';
    }
    // è¡¥å…¨æ‹¬å·
    for (let i = 0; i < braceCount; i++) {
      result += '}';
    }
    return result;
  }

  return null;
}

/**
 * åˆ›å»ºå¸¦çŠ¶æ€ç çš„é”™è¯¯å¯¹è±¡ï¼Œä¾¿äºé‡è¯•é€»è¾‘åˆ¤æ–­
 */
function createHttpError(status: number, message: string, response?: Response): Error & { status: number; response?: { headers: Record<string, string> } } {
  const error = new Error(message) as Error & { status: number; response?: { headers: Record<string, string> } };
  error.status = status;

  // å°è¯•è§£æ Retry-After å¤´ï¼Œä¼ é€’ç»™é‡è¯•é€»è¾‘
  if (response) {
    const retryAfter = response.headers.get('retry-after');
    if (retryAfter) {
      error.response = {
        headers: { 'retry-after': retryAfter }
      };
    }
  }

  return error;
}

/**
 * åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•è‡ªå®šä¹‰æ¨¡å‹è¯·æ±‚
 * é‡è¯•æ¡ä»¶ï¼š429 é™æµ æˆ– 5xx æœåŠ¡å™¨é”™è¯¯
 */
function shouldRetryCustomModel(error: Error): boolean {
  const status = getErrorStatus(error);

  // âœ… 429 é™æµ - é‡è¯•
  if (status === 429) {
    console.warn(`[CustomModel] Rate limited (429), will retry with backoff...`);
    return true;
  }

  // âœ… 5xx æœåŠ¡å™¨é”™è¯¯ - é‡è¯•
  if (status && status >= 500 && status < 600) {
    console.warn(`[CustomModel] Server error (${status}), will retry...`);
    return true;
  }

  // âœ… æ£€æŸ¥é”™è¯¯æ¶ˆæ¯ä¸­çš„ 429
  if (error.message.includes('429')) {
    console.warn(`[CustomModel] Rate limit detected in message, will retry...`);
    return true;
  }

  // âŒ å…¶ä»–é”™è¯¯ï¼ˆå¦‚ 4xx å®¢æˆ·ç«¯é”™è¯¯ï¼‰ä¸é‡è¯•
  return false;
}



/**
 * OpenAI æ ¼å¼è½¬æ¢å·¥å…·
 */
const OpenAIConverter = {
  /**
   * å°†å•ä¸ª part è½¬æ¢ä¸º OpenAI content æ ¼å¼
   * æ”¯æŒ text å’Œ inlineData (å›¾ç‰‡)
   */
  partToOpenAIContent(part: any): any | null {
    if (part.text) {
      return { type: 'text', text: part.text };
    }
    if (part.inlineData) {
      // è½¬æ¢ Gemini inlineData æ ¼å¼ä¸º OpenAI image_url æ ¼å¼
      const { mimeType, data } = part.inlineData;
      return {
        type: 'image_url',
        image_url: {
          url: `data:${mimeType};base64,${data}`,
        },
      };
    }
    return null;
  },

  contentsToMessages(contents: any[]): any[] {
    return contents.map((content: any) => {
      const parts = content.parts || [];

      if (parts.some((p: any) => p.functionCall)) {
        return {
          role: content.role === MESSAGE_ROLES.MODEL ? 'assistant' : 'user',
          content: null,
          tool_calls: parts
            .filter((p: any) => p.functionCall)
            .map((p: any, idx: number) => ({
              id: p.functionCall.id || `call_${Date.now()}_${idx}`,
              type: 'function',
              function: {
                name: p.functionCall.name,
                arguments: typeof p.functionCall.args === 'string'
                  ? p.functionCall.args
                  : JSON.stringify(p.functionCall.args || {}),
              },
            })),
        };
      }

      if (parts.some((p: any) => p.functionResponse)) {
        const functionResponseParts = parts.filter((p: any) => p.functionResponse);
        return functionResponseParts.map((p: any) => ({
          role: 'tool',
          tool_call_id: p.functionResponse.id || `call_${p.functionResponse.name}`,
          content: typeof p.functionResponse.response === 'string'
            ? p.functionResponse.response
            : JSON.stringify(p.functionResponse.response || {}),
        }));
      }

      // æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡å†…å®¹
      const hasImageContent = parts.some((p: any) => p.inlineData);

      if (hasImageContent) {
        // ä½¿ç”¨æ•°ç»„æ ¼å¼ä»¥æ”¯æŒæ··åˆå†…å®¹ï¼ˆæ–‡æœ¬ + å›¾ç‰‡ï¼‰
        const contentParts = parts
          .map((part: any) => OpenAIConverter.partToOpenAIContent(part))
          .filter(Boolean);

        return {
          role: content.role === MESSAGE_ROLES.MODEL ? 'assistant' : 'user',
          content: contentParts,
        };
      }

      // çº¯æ–‡æœ¬å†…å®¹ï¼Œä½¿ç”¨ç®€å•å­—ç¬¦ä¸²æ ¼å¼
      return {
        role: content.role === MESSAGE_ROLES.MODEL ? 'assistant' : 'user',
        content: parts.map((part: any) => part.text || '').join('\n'),
      };
    }).flat();
  },

  toolsToOpenAITools(tools: any[]): any[] | undefined {
    if (!tools || tools.length === 0) return undefined;
    return tools.flatMap((tool: any) => {
      if (tool.functionDeclarations && Array.isArray(tool.functionDeclarations)) {
        return tool.functionDeclarations.map((fd: any) => ({
          type: 'function',
          function: {
            name: fd.name,
            description: fd.description,
            parameters: fd.parameters,
          },
        }));
      }
      return [{
        type: 'function',
        function: {
          name: tool.name,
          description: tool.description,
          parameters: tool.parameters,
        },
      }];
    });
  },

  mapFinishReason(reason: string): FinishReason {
    switch (reason) {
      case 'stop': return FinishReason.STOP;
      case 'length': return FinishReason.MAX_TOKENS;
      case 'content_filter': return FinishReason.SAFETY;
      case 'tool_calls': return FinishReason.STOP;
      default: return FinishReason.OTHER;
    }
  }
};

/**
 * Anthropic æ ¼å¼è½¬æ¢å·¥å…·
 * å®Œæ•´æ”¯æŒ Anthropic Messages API æ ¼å¼ï¼ŒåŒ…æ‹¬ï¼š
 * - system æ•°ç»„æ ¼å¼ï¼ˆå¸¦ cache_controlï¼‰
 * - extended thinking é…ç½®
 * - å®Œæ•´çš„ input_schemaï¼ˆå« additionalPropertiesï¼‰
 * @see https://docs.anthropic.com/en/api/messages
 */
const AnthropicConverter = {
  /**
   * å°† Gemini æ ¼å¼å†…å®¹è½¬æ¢ä¸º Anthropic æ ¼å¼
   * è‡ªåŠ¨æ·»åŠ  cache_control ä»¥åˆ©ç”¨ Anthropic prompt cachingï¼š
   * - æ‰€æœ‰ system æ¶ˆæ¯å—æ·»åŠ  cache_control: { type: 'ephemeral' }
   * - ç”¨æˆ·æ¶ˆæ¯çš„æœ€åä¸€ä¸ªæ–‡æœ¬å—æ·»åŠ  cache_control: { type: 'ephemeral' }
   * @see https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching
   */
  contentsToAnthropic(contents: any[]): { messages: any[], system?: any[] } {
    const messages: any[] = [];
    const systemBlocks: any[] = [];

    for (const content of contents) {
      const parts = content.parts || [];

      if (content.role === 'system') {
        // è½¬æ¢ä¸º Anthropic system æ•°ç»„æ ¼å¼
        for (const p of parts) {
          if (p.text) {
            const block: any = { type: 'text', text: p.text };
            // ğŸ†• è‡ªåŠ¨æ·»åŠ  cache_controlï¼ˆä¸ Claude Code è¡Œä¸ºä¸€è‡´ï¼‰
            block.cache_control = p.cache_control || { type: 'ephemeral' };
            systemBlocks.push(block);
          }
        }
        continue;
      }

      const role = content.role === MESSAGE_ROLES.MODEL ? 'assistant' : 'user';
      const anthropicParts: any[] = [];

      for (const part of parts) {
        if (part.text) {
          const textBlock: any = { type: 'text', text: part.text };
          // é€ä¼ å·²æœ‰çš„ cache_controlï¼ˆåç»­ä¼šä¸ºæœ€åä¸€ä¸ªæ–‡æœ¬å—è‡ªåŠ¨æ·»åŠ ï¼‰
          if (part.cache_control) {
            textBlock.cache_control = part.cache_control;
          }
          anthropicParts.push(textBlock);
        }
        if (part.inlineData) {
          // è½¬æ¢ Gemini inlineData æ ¼å¼ä¸º Anthropic image æ ¼å¼
          anthropicParts.push({
            type: 'image',
            source: {
              type: 'base64',
              media_type: part.inlineData.mimeType,
              data: part.inlineData.data,
            },
          });
        }
        if (part.functionCall) {
          anthropicParts.push({
            type: 'tool_use',
            id: part.functionCall.id || `toolu_${Date.now()}_${Math.random().toString(36).slice(2)}`,
            name: part.functionCall.name,
            input: part.functionCall.args || {},
          });
        }
        if (part.functionResponse) {
          anthropicParts.push({
            type: 'tool_result',
            tool_use_id: part.functionResponse.id || `toolu_${part.functionResponse.name}`,
            content: typeof part.functionResponse.response === 'string'
              ? part.functionResponse.response
              : JSON.stringify(part.functionResponse.response || {}),
          });
        }
      }

      if (anthropicParts.length > 0) {
        messages.push({ role, content: anthropicParts });
      }
    }

    if (messages.length > 0 && messages[0].role === 'assistant') {
      messages.unshift({ role: 'user', content: '...' });
    }

    const merged: any[] = [];
    for (const msg of messages) {
      const prev = merged[merged.length - 1];
      if (prev && prev.role === msg.role) {
        const prevContent = Array.isArray(prev.content) ? prev.content : [{type:'text', text: prev.content}];
        const msgContent = Array.isArray(msg.content) ? msg.content : [{type:'text', text: msg.content}];
        prev.content = [...prevContent, ...msgContent];
      } else {
        merged.push(msg);
      }
    }

    // ğŸ†• ä¸ºæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„æœ€åä¸€ä¸ªæ–‡æœ¬å—æ·»åŠ  cache_control
    // ä¸ Claude Code è¡Œä¸ºä¸€è‡´ï¼Œåˆ©ç”¨ prompt caching å‡å°‘ token æ¶ˆè€—
    for (let i = merged.length - 1; i >= 0; i--) {
      if (merged[i].role === 'user' && Array.isArray(merged[i].content)) {
        const content = merged[i].content;
        // æ‰¾åˆ°æœ€åä¸€ä¸ªæ–‡æœ¬å—
        for (let j = content.length - 1; j >= 0; j--) {
          if (content[j].type === 'text' && !content[j].cache_control) {
            content[j].cache_control = { type: 'ephemeral' };
            break;
          }
        }
        break; // åªå¤„ç†æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
      }
    }

    return {
      messages: merged,
      system: systemBlocks.length > 0 ? systemBlocks : undefined
    };
  },

  /**
   * å°†å·¥å…·å®šä¹‰è½¬æ¢ä¸º Anthropic æ ¼å¼
   * å®Œæ•´æ”¯æŒ input_schemaï¼ˆå« additionalProperties: falseï¼‰
   */
  toolsToAnthropicTools(tools: any[]): any[] | undefined {
    if (!tools || tools.length === 0) return undefined;

    const cleanSchema = (schema: any, isRoot: boolean = false): any => {
      if (!schema || typeof schema !== 'object') return schema;
      const cleaned: any = {};
      const validFields = ['type', 'properties', 'required', 'items', 'enum', 'description', 'default', 'minimum', 'maximum', 'exclusiveMinimum', 'exclusiveMaximum', 'minLength', 'maxLength', 'pattern', 'format', 'minItems', 'maxItems', 'uniqueItems', 'additionalProperties', 'anyOf', 'oneOf', 'allOf', 'not'];
      for (const key of validFields) {
        if (schema[key] !== undefined) {
          if (key === 'type' && typeof schema[key] === 'string') cleaned[key] = schema[key].toLowerCase();
          else if (['minimum', 'maximum', 'exclusiveMinimum', 'exclusiveMaximum', 'minLength', 'maxLength', 'minItems', 'maxItems'].includes(key)) {
            const val = parseFloat(schema[key]);
            if (!isNaN(val)) cleaned[key] = val;
          }
          else if (key === 'properties' && typeof schema[key] === 'object') {
            cleaned[key] = {};
            for (const k in schema[key]) cleaned[key][k] = cleanSchema(schema[key][k], false);
          } else if (key === 'items') cleaned[key] = cleanSchema(schema[key], false);
          else cleaned[key] = schema[key];
        }
      }
      return cleaned;
    };

    return tools.flatMap((tool: any) => {
      const decls = tool.functionDeclarations || [tool];
      return decls.map((fd: any) => {
        const cleaned = cleanSchema(fd.parameters || {}, true);
        return {
          name: fd.name,
          description: fd.description || '',
          input_schema: {
            $schema: 'https://json-schema.org/draft/2020-12/schema',
            type: 'object',
            properties: cleaned.properties || {},
            ...(cleaned.required && { required: cleaned.required }),
            // ğŸ”§ å…³é”®ï¼šæ·»åŠ  additionalProperties: false ä»¥åŒ¹é… Claude Code çš„è¡Œä¸º
            additionalProperties: false,
          },
        };
      });
    });
  },

  mapFinishReason(reason: string): FinishReason {
    switch (reason) {
      case 'end_turn': return FinishReason.STOP;
      case 'max_tokens': return FinishReason.MAX_TOKENS;
      case 'tool_use': return FinishReason.STOP;
      default: return FinishReason.OTHER;
    }
  }
};

/**
 * OpenAI å…¼å®¹æ¨¡å‹å•æ¬¡è°ƒç”¨
 * ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥å¤„ç† 429 å’Œ 5xx é”™è¯¯
 */
export async function callOpenAICompatibleModel(
  modelConfig: CustomModelConfig,
  request: any,
  abortSignal?: AbortSignal
): Promise<GenerateContentResponse> {
  const baseUrl = resolveEnvVar(modelConfig.baseUrl).replace(/\/+$/, '');
  const apiKey = resolveEnvVar(modelConfig.apiKey);
  const url = `${baseUrl}/chat/completions`;

  const requestBody: any = {
    model: modelConfig.modelId,
    messages: OpenAIConverter.contentsToMessages(request.contents),
    tools: OpenAIConverter.toolsToOpenAITools(request.config?.tools),
    stream: false,
  };

  // ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•åŒ…è£… API è°ƒç”¨
  return retryWithBackoff(
    async () => {
      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`,
          ...modelConfig.headers,
        },
        body: JSON.stringify(requestBody),
        signal: abortSignal,
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw createHttpError(response.status, `OpenAI API error (${response.status}): ${errorText}`, response);
      }

      const data = await response.json();
      const choice = data.choices[0];
      const message = choice.message;

      const parts: any[] = [];
      if (message.content) parts.push({ text: message.content });
      if (message.tool_calls) {
        for (const tc of message.tool_calls) {
          if (tc.type === 'function') {
            parts.push({
              functionCall: {
                name: tc.function.name,
                args: parseJSONSafe(tc.function.arguments),
                id: tc.id,
              },
            });
          }
        }
      }

      // ğŸ”§ OpenAI prompt cachingï¼šç¼“å­˜ä¿¡æ¯åœ¨ usage.prompt_tokens_details.cached_tokens
      // å‚è€ƒï¼šhttps://platform.openai.com/docs/guides/prompt-caching
      const cachedTokens = data.usage?.prompt_tokens_details?.cached_tokens || 0;
      const promptTokens = data.usage?.prompt_tokens || 0;

      const result = {
        candidates: [{
          content: { role: MESSAGE_ROLES.MODEL, parts: parts.length ? parts : [{ text: '' }] },
          finishReason: OpenAIConverter.mapFinishReason(choice.finish_reason),
          index: 0,
        }],
        usageMetadata: {
          promptTokenCount: promptTokens,
          candidatesTokenCount: data.usage?.completion_tokens || 0,
          totalTokenCount: data.usage?.total_tokens || 0,
          // ğŸ”§ OpenAI prompt caching support
          // OpenAI ä½¿ç”¨ prompt_tokens_details.cached_tokens è¡¨ç¤ºç¼“å­˜å‘½ä¸­çš„ token
          // æ˜ å°„åˆ°æˆ‘ä»¬çš„å­—æ®µåä»¥ä¿æŒä¸ geminiChat.ts å…¼å®¹
          ...(cachedTokens > 0 && { cacheReadInputTokens: cachedTokens }),
          // OpenAI ä¸åŒºåˆ† cache creationï¼Œåªæœ‰ cache read
          // uncachedInputTokens = promptTokens - cachedTokens
          uncachedInputTokens: promptTokens - cachedTokens,
        } as any,
      };
      addFunctionCallsGetter(result);
      return result as GenerateContentResponse;
    },
    {
      shouldRetry: shouldRetryCustomModel,
    }
  );
}

/**
 * æ£€æŸ¥æ˜¯å¦åº”è¯¥å¯ç”¨ Extended Thinking
 * å¯¹äº Anthropic åè®®ï¼Œé»˜è®¤å¯ç”¨ thinkingï¼ˆè®©æœåŠ¡ç«¯å†³å®šæ˜¯å¦æ”¯æŒï¼‰
 * ä¸æ”¯æŒçš„æ¨¡å‹ä¼šå¿½ç•¥æ­¤å‚æ•°ï¼Œå› æ­¤ç»Ÿä¸€å¯ç”¨æ›´ç®€å•é€šç”¨
 * @see https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking
 */
function shouldEnableThinkingByDefault(): boolean {
  // å¯¹äºæ‰€æœ‰ Anthropic åè®®çš„æ¨¡å‹ï¼Œé»˜è®¤å¯ç”¨ thinking
  // å¦‚æœæ¨¡å‹ä¸æ”¯æŒï¼ŒæœåŠ¡ç«¯ä¼šè‡ªåŠ¨å¿½ç•¥æ­¤å‚æ•°
  return true;
}

/**
 * Anthropic æ¨¡å‹å•æ¬¡è°ƒç”¨
 * ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥å¤„ç† 429 å’Œ 5xx é”™è¯¯
 * æ”¯æŒ extended thinking é…ç½®
 */
export async function callAnthropicModel(
  modelConfig: CustomModelConfig,
  request: any,
  abortSignal?: AbortSignal
): Promise<GenerateContentResponse> {
  const baseUrl = resolveEnvVar(modelConfig.baseUrl).replace(/\/+$/, '');
  const apiKey = resolveEnvVar(modelConfig.apiKey);
  const { messages, system } = AnthropicConverter.contentsToAnthropic(request.contents);

  const requestBody: any = {
    model: modelConfig.modelId,
    messages,
    tools: AnthropicConverter.toolsToAnthropicTools(request.config?.tools),
    max_tokens: modelConfig.maxTokens || 4096,
  };

  // æ·»åŠ  systemï¼ˆæ•°ç»„æ ¼å¼ï¼Œå¸¦ cache_control æ”¯æŒï¼‰
  if (system && system.length > 0) {
    requestBody.system = system;
  }

  // ğŸ†• Extended Thinking æ™ºèƒ½å¯ç”¨ç­–ç•¥ï¼š
  // 1. å¦‚æœç”¨æˆ·æ˜ç¡®è®¾ç½®äº† enableThinkingï¼Œéµå¾ªç”¨æˆ·é…ç½®
  // 2. å¦‚æœç”¨æˆ·æœªè®¾ç½®ï¼ˆundefinedï¼‰ï¼Œé»˜è®¤å¯ç”¨ï¼ˆæ‰€æœ‰ Anthropic åè®®ï¼‰
  // 3. ä¸æ”¯æŒçš„æ¨¡å‹ä¼šè‡ªåŠ¨å¿½ç•¥ thinking å‚æ•°ï¼Œå› æ­¤ç»Ÿä¸€å¯ç”¨æ›´ç®€å•
  const shouldEnableThinking = modelConfig.enableThinking !== undefined
    ? modelConfig.enableThinking
    : shouldEnableThinkingByDefault();

  if (shouldEnableThinking) {
    const maxTokens = modelConfig.maxTokens || 32000; // æ€è€ƒæ¨¡å¼å»ºè®®ä½¿ç”¨è¾ƒå¤§çš„ max_tokens
    requestBody.thinking = {
      type: 'enabled',
      budget_tokens: Math.min(maxTokens - 1, 31999), // budget_tokens å¿…é¡»å°äº max_tokensï¼Œé»˜è®¤ä½¿ç”¨å®˜æ–¹æ¨èçš„ 31999
    };
    // ç¡®ä¿ max_tokens è¶³å¤Ÿå¤§ä»¥å®¹çº³ thinking + å›å¤
    requestBody.max_tokens = Math.max(maxTokens, 32000);
  }

  // ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•åŒ…è£… API è°ƒç”¨
  return retryWithBackoff(
    async () => {
      const response = await fetch(`${baseUrl}/v1/messages`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-api-key': apiKey,
          'anthropic-version': '2023-06-01',
          ...modelConfig.headers,
        },
        body: JSON.stringify(requestBody),
        signal: abortSignal,
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw createHttpError(response.status, `Anthropic error (${response.status}): ${errorText}`, response);
      }

      const data = await response.json();
      const parts = data.content.map((c: any) => {
        if (c.type === 'text') return { text: c.text };
        if (c.type === 'tool_use') return { functionCall: { name: c.name, args: c.input, id: c.id } };
        // ğŸ†• æ”¯æŒ thinking å†…å®¹å— - æ˜ å°„ä¸º reasoning æ ¼å¼ä»¥ä¾¿ UI æ˜¾ç¤º
        // Anthropic çš„ thinking å—åŒ…å«æ¨¡å‹çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ï¼Œç±»ä¼¼äº Gemini çš„ reasoning å­—æ®µ
        if (c.type === 'thinking') return { reasoning: c.thinking };
        return null;
      }).filter(Boolean);

      // ğŸ”§ è®¡ç®—çœŸæ­£çš„æ€»è¾“å…¥ tokenï¼š
      // Anthropic çš„ input_tokens åªæ˜¯éç¼“å­˜çš„ç›´æ¥è¾“å…¥ï¼Œå®é™…æ€»è¾“å…¥éœ€è¦åŠ ä¸Šç¼“å­˜ token
      const uncachedInputTokens = data.usage?.input_tokens || 0;
      const cacheCreationTokens = data.usage?.cache_creation_input_tokens || 0;
      const cacheReadTokens = data.usage?.cache_read_input_tokens || 0;
      const actualPromptTokens = uncachedInputTokens + cacheCreationTokens + cacheReadTokens;
      const outputTokens = data.usage?.output_tokens || 0;

      const result = {
        candidates: [{
          content: { role: MESSAGE_ROLES.MODEL, parts: parts.length ? parts : [{ text: '' }] },
          finishReason: AnthropicConverter.mapFinishReason(data.stop_reason),
          index: 0,
        }],
        usageMetadata: {
          // promptTokenCount åº”è¯¥åæ˜ å®é™…å¤„ç†çš„æ€»è¾“å…¥ tokenï¼ˆåŒ…æ‹¬ç¼“å­˜ï¼‰
          promptTokenCount: actualPromptTokens,
          candidatesTokenCount: outputTokens,
          totalTokenCount: actualPromptTokens + outputTokens,
          // ğŸ”§ Claude prompt caching è¯¦ç»†ä¿¡æ¯
          // å­—æ®µåä¸ geminiChat.ts ä¸­è¯»å–çš„ä¸€è‡´ï¼ˆä¸å¸¦ Count åç¼€ï¼‰
          // - cacheCreationInputTokens: æœ¬æ¬¡å†™å…¥ç¼“å­˜çš„ tokenï¼ˆ1.25x ä»·æ ¼ï¼‰
          // - cacheReadInputTokens: ä»ç¼“å­˜è¯»å–çš„ tokenï¼ˆ0.1x ä»·æ ¼ï¼Œä¾¿å®œ 90%ï¼‰
          // - uncachedInputTokens: éç¼“å­˜çš„ç›´æ¥è¾“å…¥ token
          ...(cacheCreationTokens && { cacheCreationInputTokens: cacheCreationTokens }),
          ...(cacheReadTokens != null && { cacheReadInputTokens: cacheReadTokens }),
          uncachedInputTokens: uncachedInputTokens,
        } as any,
      };
      addFunctionCallsGetter(result);
      return result as GenerateContentResponse;
    },
    {
      shouldRetry: shouldRetryCustomModel,
    }
  );
}

/**
 * OpenAI å…¼å®¹æ¨¡å‹æµå¼è°ƒç”¨
 * ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥å¤„ç†åˆå§‹è¿æ¥çš„ 429 å’Œ 5xx é”™è¯¯
 */
export async function* callOpenAICompatibleModelStream(
  modelConfig: CustomModelConfig,
  request: any,
  abortSignal?: AbortSignal
): AsyncGenerator<GenerateContentResponse> {
  const baseUrl = resolveEnvVar(modelConfig.baseUrl).replace(/\/+$/, '');
  const apiKey = resolveEnvVar(modelConfig.apiKey);

  const requestBody: any = {
    model: modelConfig.modelId,
    messages: OpenAIConverter.contentsToMessages(request.contents),
    tools: OpenAIConverter.toolsToOpenAITools(request.config?.tools),
    stream: true,
    stream_options: { include_usage: true } // è¯·æ±‚åŒ…å« usage ä¿¡æ¯
  };

  // ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•åŒ…è£…åˆå§‹è¿æ¥
  const response = await retryWithBackoff(
    async () => {
      const res = await fetch(`${baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`,
          ...modelConfig.headers,
        },
        body: JSON.stringify(requestBody),
        signal: abortSignal,
      });

      if (!res.ok) {
        const errorText = await res.text();
        throw createHttpError(res.status, `OpenAI Stream error (${res.status}): ${errorText}`, res);
      }

      return res;
    },
    {
      shouldRetry: shouldRetryCustomModel,
    }
  );

  const reader = response.body?.getReader();
  if (!reader) throw new Error('No response body');

  const decoder = new TextDecoder();
  let buffer = '';
  // ç”¨äºèšåˆæµå¼å·¥å…·è°ƒç”¨
  const aggregatedTools: Map<number, { id: string, name: string, args: string }> = new Map();

  const flushTools = function* (): Generator<GenerateContentResponse> {
    if (aggregatedTools.size === 0) return;
    const toolParts = Array.from(aggregatedTools.values()).map(at => ({
      functionCall: {
        name: at.name || 'unknown_tool',
        args: parseJSONSafe(at.args),
        id: at.id || `call_${Date.now()}`
      }
    }));
    const content = { role: MESSAGE_ROLES.MODEL, parts: toolParts };
    const resp = {
      candidates: [{
        content,
        finishReason: FinishReason.STOP,
        index: 0
      }]
    };
    addFunctionCallsGetter(resp);
    addFunctionCallsGetter(content);
    yield resp as GenerateContentResponse;
    aggregatedTools.clear();
  };

  try {
    let isDone = false;
    while (true) {
      const { done, value } = await reader.read();
      if (done) {
        isDone = true;
      }

      if (!done) {
        buffer += decoder.decode(value, { stream: true });
      } else {
        // æµç»“æŸï¼Œä½¿ç”¨æœ€ç»ˆè§£ç 
        buffer += decoder.decode(undefined, { stream: false });
      }

      const lines = buffer.split('\n');
      buffer = lines.pop() || '';

      for (const line of lines) {
        const trimmed = line.trim();
        if (!trimmed || !trimmed.startsWith('data: ')) continue;
        const dataStr = trimmed.slice(6);
        if (dataStr === '[DONE]') {
          // OpenAI æ˜ç¡®è¡¨ç¤ºæµç»“æŸï¼Œæ­¤æ—¶åº”è¯¥ flush æ‰€æœ‰å¾…å®Œæˆçš„å·¥å…·è°ƒç”¨
          yield* flushTools();
          isDone = true;
          break;
        }

        try {
          const chunk = JSON.parse(dataStr);
          const choice = chunk.choices?.[0];

          if (choice) {
            const delta = choice.delta;

            // å¤„ç†æ–‡æœ¬å†…å®¹ - ç«‹å³ yield
            if (delta?.content) {
              const content = { role: MESSAGE_ROLES.MODEL, parts: [{ text: delta.content }] };
              const resp = { candidates: [{ content, index: 0 }] };
              addFunctionCallsGetter(resp);
              addFunctionCallsGetter(content);
              yield resp as GenerateContentResponse;
            }

            // èšåˆå·¥å…·è°ƒç”¨ - ä¸ç«‹å³ yieldï¼Œç­‰å¾…å®Œå…¨æ¥æ”¶
            if (delta?.tool_calls) {
              for (const tc of delta.tool_calls) {
                const idx = tc.index ?? 0;
                let tool = aggregatedTools.get(idx);
                if (!tool) {
                  tool = { id: '', name: '', args: '' };
                  aggregatedTools.set(idx, tool);
                }
                if (tc.id) tool.id = tc.id;
                if (tc.function?.name) tool.name = tc.function.name;
                if (tc.function?.arguments) tool.args += tc.function.arguments;
              }
            }

            // åªåœ¨æµç»“æŸæ—¶ flushï¼Œä¸åœ¨ finish_reason ä¸­é—´ flush
            // è¿™ä¸ Claude çš„è¡Œä¸ºä¸€è‡´ï¼Œé˜²æ­¢ä¸å®Œæ•´çš„å·¥å…·è°ƒç”¨è¢«è¯†åˆ«
          }

          if (chunk.usage) {
            // ğŸ”§ OpenAI prompt cachingï¼šç¼“å­˜ä¿¡æ¯åœ¨ usage.prompt_tokens_details.cached_tokens
            const cachedTokens = chunk.usage.prompt_tokens_details?.cached_tokens || 0;
            const promptTokens = chunk.usage.prompt_tokens || 0;

            yield {
              candidates: [],
              usageMetadata: {
                promptTokenCount: promptTokens,
                candidatesTokenCount: chunk.usage.completion_tokens || 0,
                totalTokenCount: chunk.usage.total_tokens || 0,
                // ğŸ”§ OpenAI prompt caching support
                // OpenAI ä½¿ç”¨ prompt_tokens_details.cached_tokens è¡¨ç¤ºç¼“å­˜å‘½ä¸­çš„ token
                // æ˜ å°„åˆ°æˆ‘ä»¬çš„å­—æ®µåä»¥ä¿æŒä¸ geminiChat.ts å…¼å®¹
                ...(cachedTokens > 0 && { cacheReadInputTokens: cachedTokens }),
                // OpenAI ä¸åŒºåˆ† cache creationï¼Œåªæœ‰ cache read
                uncachedInputTokens: promptTokens - cachedTokens,
              }
            } as any;
          }
        } catch (e) {}
      }

      if (isDone) {
        // åœ¨æµå®Œå…¨ç»“æŸæ—¶ï¼Œflush æ‰€æœ‰å¾…å®Œæˆçš„å·¥å…·è°ƒç”¨
        yield* flushTools();
        break;
      }
    }
  } finally {
    reader.releaseLock();
  }
}

/**
 * Anthropic æ¨¡å‹æµå¼è°ƒç”¨
 * ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥å¤„ç†åˆå§‹è¿æ¥çš„ 429 å’Œ 5xx é”™è¯¯
 * æ”¯æŒ extended thinking é…ç½®
 */
export async function* callAnthropicModelStream(
  modelConfig: CustomModelConfig,
  request: any,
  abortSignal?: AbortSignal
): AsyncGenerator<GenerateContentResponse> {
  const baseUrl = resolveEnvVar(modelConfig.baseUrl).replace(/\/+$/, '');
  const apiKey = resolveEnvVar(modelConfig.apiKey);
  const { messages, system } = AnthropicConverter.contentsToAnthropic(request.contents);

  const requestBody: any = {
    model: modelConfig.modelId,
    messages,
    tools: AnthropicConverter.toolsToAnthropicTools(request.config?.tools),
    max_tokens: modelConfig.maxTokens || 4096,
    stream: true,
  };

  // æ·»åŠ  systemï¼ˆæ•°ç»„æ ¼å¼ï¼Œå¸¦ cache_control æ”¯æŒï¼‰
  if (system && system.length > 0) {
    requestBody.system = system;
  }

  // ğŸ†• Extended Thinking æ™ºèƒ½å¯ç”¨ç­–ç•¥ï¼ˆæµå¼è°ƒç”¨ï¼‰ï¼š
  // 1. å¦‚æœç”¨æˆ·æ˜ç¡®è®¾ç½®äº† enableThinkingï¼Œéµå¾ªç”¨æˆ·é…ç½®
  // 2. å¦‚æœç”¨æˆ·æœªè®¾ç½®ï¼ˆundefinedï¼‰ï¼Œé»˜è®¤å¯ç”¨ï¼ˆæ‰€æœ‰ Anthropic åè®®ï¼‰
  // 3. ä¸æ”¯æŒçš„æ¨¡å‹ä¼šè‡ªåŠ¨å¿½ç•¥ thinking å‚æ•°ï¼Œå› æ­¤ç»Ÿä¸€å¯ç”¨æ›´ç®€å•
  const shouldEnableThinking = modelConfig.enableThinking !== undefined
    ? modelConfig.enableThinking
    : shouldEnableThinkingByDefault();

  if (shouldEnableThinking) {
    const maxTokens = modelConfig.maxTokens || 32000; // æ€è€ƒæ¨¡å¼å»ºè®®ä½¿ç”¨è¾ƒå¤§çš„ max_tokens
    requestBody.thinking = {
      type: 'enabled',
      budget_tokens: Math.min(maxTokens - 1, 31999), // budget_tokens å¿…é¡»å°äº max_tokensï¼Œé»˜è®¤ä½¿ç”¨å®˜æ–¹æ¨èçš„ 31999
    };
    // ç¡®ä¿ max_tokens è¶³å¤Ÿå¤§ä»¥å®¹çº³ thinking + å›å¤
    requestBody.max_tokens = Math.max(maxTokens, 32000);
  }

  // ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•åŒ…è£…åˆå§‹è¿æ¥
  const response = await retryWithBackoff(
    async () => {
      const res = await fetch(`${baseUrl}/v1/messages`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-api-key': apiKey,
          'anthropic-version': '2023-06-01',
          ...modelConfig.headers,
        },
        body: JSON.stringify(requestBody),
        signal: abortSignal,
      });

      if (!res.ok) {
        const errorText = await res.text();
        throw createHttpError(res.status, `Anthropic Stream error (${res.status}): ${errorText}`, res);
      }

      return res;
    },
    {
      shouldRetry: shouldRetryCustomModel,
    }
  );

  const reader = response.body?.getReader();
  if (!reader) throw new Error('No response body');

  const decoder = new TextDecoder();
  let buffer = '';
  const aggregatedTools: Map<number, { id: string, name: string, args: string }> = new Map();
  // ğŸ†• ç”¨äºèšåˆ thinking å†…å®¹å—ï¼ˆæµå¼ç´¯ç§¯åä¸€æ¬¡æ€§å‘é€ï¼‰
  const aggregatedThinking: Map<number, string> = new Map();

  // ç”¨äºç´¯ç§¯ token ä½¿ç”¨ç»Ÿè®¡
  // ğŸ”§ ä¿®å¤ï¼šç¼“å­˜ token æ¥è‡ª message_startï¼ˆåˆå§‹å€¼ï¼‰ï¼Œoutput_tokens æ¥è‡ª message_deltaï¼ˆç´¯åŠ ï¼‰
  let inputTokens = 0;
  let totalOutputTokens = 0;
  // ç¼“å­˜ç›¸å…³ tokenï¼ˆä» message_start è·å–ï¼Œä¸ç´¯åŠ ï¼‰
  let cacheCreationInputTokens = 0;
  let cacheReadInputTokens = 0;

  try {
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      buffer += decoder.decode(value, { stream: true });
      const lines = buffer.split('\n');
      buffer = lines.pop() || '';

      for (const line of lines) {
        const trimmed = line.trim();
        if (!trimmed || !trimmed.startsWith('data: ')) continue;
        const dataStr = trimmed.slice(6);

        try {
          const chunk = JSON.parse(dataStr);
          const idx = chunk.index ?? 0;

          if (chunk.type === 'content_block_start') {
            if (chunk.content_block?.type === 'tool_use') {
              aggregatedTools.set(idx, {
                id: chunk.content_block.id,
                name: chunk.content_block.name,
                args: ''
              });
            } else if (chunk.content_block?.type === 'thinking') {
              // ğŸ†• å¼€å§‹èšåˆ thinking å†…å®¹å—
              aggregatedThinking.set(idx, chunk.content_block.thinking || '');
            }
          } else if (chunk.type === 'content_block_delta') {
            if (chunk.delta?.type === 'text_delta') {
              const content = { role: MESSAGE_ROLES.MODEL, parts: [{ text: chunk.delta.text }] };
              const resp = { candidates: [{ content, index: 0 }] };
              addFunctionCallsGetter(resp);
              addFunctionCallsGetter(content);
              yield resp as any;
            } else if (chunk.delta?.type === 'input_json_delta') {
              const tool = aggregatedTools.get(idx);
              if (tool) tool.args += chunk.delta.partial_json;
            } else if (chunk.delta?.type === 'thinking_delta') {
              // ğŸ†• å®æ—¶æµå¼è¾“å‡º thinking å†…å®¹ï¼Œè®© UI èƒ½æ˜¾ç¤ºæ¨¡å‹æ€è€ƒè¿‡ç¨‹
              const thinkingChunk = chunk.delta.thinking || '';
              if (thinkingChunk) {
                const content = { role: MESSAGE_ROLES.MODEL, parts: [{ reasoning: thinkingChunk }] } as any;
                const resp = { candidates: [{ content, index: 0 }] } as any;
                addFunctionCallsGetter(resp);
                addFunctionCallsGetter(content);
                yield resp;
              }
              // åŒæ—¶ç´¯ç§¯å®Œæ•´å†…å®¹ï¼Œä»¥ä¾¿åœ¨ content_block_stop æ—¶å¯ç”¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
              const existing = aggregatedThinking.get(idx) || '';
              aggregatedThinking.set(idx, existing + thinkingChunk);
            }
          } else if (chunk.type === 'content_block_stop') {
            const tool = aggregatedTools.get(idx);
            if (tool) {
              const content = { role: MESSAGE_ROLES.MODEL, parts: [{ functionCall: { name: tool.name, args: parseJSONSafe(tool.args), id: tool.id } }] };
              const resp = {
                candidates: [{
                  content,
                  index: 0
                }]
              };
              addFunctionCallsGetter(resp);
              addFunctionCallsGetter(content);
              yield resp as GenerateContentResponse;
              aggregatedTools.delete(idx);
            }
            // ğŸ†• thinking å†…å®¹å·²åœ¨ thinking_delta ä¸­å®æ—¶æµå¼è¾“å‡ºï¼Œè¿™é‡Œåªéœ€æ¸…ç†çŠ¶æ€
            // ä¸å†é‡å¤ yield å®Œæ•´å†…å®¹ï¼Œé¿å… UI æ˜¾ç¤ºé‡å¤
            if (aggregatedThinking.has(idx)) {
              aggregatedThinking.delete(idx);
            }
          } else if (chunk.type === 'message_delta') {
            // ğŸ”§ message_delta ä¸­çš„ output_tokens æ˜¯æœ€ç»ˆæ€»æ•°ï¼Œä¸æ˜¯å¢é‡ï¼Œæ‰€ä»¥ç”¨æ›¿æ¢è€Œéç´¯åŠ 
            // å‚è€ƒæ—¥å¿—ï¼šmessage_start æœ‰ output_tokens:5ï¼Œmessage_delta æœ‰ output_tokens:298ï¼ˆæœ€ç»ˆå€¼ï¼‰
            if (chunk.usage?.output_tokens != null) {
              totalOutputTokens = chunk.usage.output_tokens;
            }

            // ğŸ”§ é²æ£’æ€§å¢å¼ºï¼šä¸€äº›ä¸Šæ¸¸å‚å•†ï¼ˆå¦‚ GLM-4 çš„ Anthropic å…¼å®¹æ¥å£ï¼‰åœ¨ message_start ä¸­
            // è¿”å› input_tokens: 0ï¼Œä½†åœ¨æœ€åçš„ message_delta ä¸­æ‰è¿”å›çœŸå®çš„ token ç”¨é‡ã€‚
            // è¿™é‡Œé‡‡ç”¨"æœ‰éé›¶å€¼å°±æ›´æ–°"çš„ç­–ç•¥ï¼Œç¡®ä¿èƒ½ä»ä»»ä½•ä½ç½®è·å–æ­£ç¡®çš„ token æ•°æ®ã€‚
            if (chunk.usage?.input_tokens != null && chunk.usage.input_tokens > 0) {
              inputTokens = chunk.usage.input_tokens;
            }
            if (chunk.usage?.cache_creation_input_tokens != null && chunk.usage.cache_creation_input_tokens > 0) {
              cacheCreationInputTokens = chunk.usage.cache_creation_input_tokens;
            }
            if (chunk.usage?.cache_read_input_tokens != null && chunk.usage.cache_read_input_tokens > 0) {
              cacheReadInputTokens = chunk.usage.cache_read_input_tokens;
            }

            // ğŸ”§ è®¡ç®—çœŸæ­£çš„æ€»è¾“å…¥ tokenï¼š
            // Anthropic çš„ input_tokens åªæ˜¯éç¼“å­˜çš„ç›´æ¥è¾“å…¥ï¼Œå®é™…æ€»è¾“å…¥éœ€è¦åŠ ä¸Šç¼“å­˜ token
            // å®é™…æ€»è¾“å…¥ = input_tokens + cache_creation_input_tokens + cache_read_input_tokens
            const actualPromptTokens = inputTokens + cacheCreationInputTokens + cacheReadInputTokens;

            const content = { role: MESSAGE_ROLES.MODEL, parts: [] };
            const resp = {
              candidates: [{
                content,
                finishReason: AnthropicConverter.mapFinishReason(chunk.delta?.stop_reason),
                index: 0
              }],
              usageMetadata: {
                // promptTokenCount åº”è¯¥åæ˜ å®é™…å¤„ç†çš„æ€»è¾“å…¥ tokenï¼ˆåŒ…æ‹¬ç¼“å­˜ï¼‰
                promptTokenCount: actualPromptTokens,
                candidatesTokenCount: totalOutputTokens,
                totalTokenCount: actualPromptTokens + totalOutputTokens,
                // ğŸ”§ Claude prompt caching è¯¦ç»†ä¿¡æ¯
                // å­—æ®µåä¸ geminiChat.ts ä¸­è¯»å–çš„ä¸€è‡´ï¼ˆä¸å¸¦ Count åç¼€ï¼‰
                // - cacheCreationInputTokens: æœ¬æ¬¡å†™å…¥ç¼“å­˜çš„ tokenï¼ˆ1.25x ä»·æ ¼ï¼‰
                // - cacheReadInputTokens: ä»ç¼“å­˜è¯»å–çš„ tokenï¼ˆ0.1x ä»·æ ¼ï¼Œä¾¿å®œ 90%ï¼‰
                // - uncachedInputTokens: éç¼“å­˜çš„ç›´æ¥è¾“å…¥ tokenï¼ˆåŸå§‹ input_tokensï¼‰
                ...(cacheCreationInputTokens != null && { cacheCreationInputTokens }),
                ...(cacheReadInputTokens != null && { cacheReadInputTokens }),
                // ä¿ç•™åŸå§‹çš„éç¼“å­˜è¾“å…¥ token ä»¥ä¾¿ç²¾ç¡®è®¡è´¹
                uncachedInputTokens: inputTokens,
              }
            } as any;
            addFunctionCallsGetter(resp);
            addFunctionCallsGetter(content);
            yield resp;
          } else if (chunk.type === 'message_start' && chunk.message?.usage) {
            // ğŸ”§ message_start åŒ…å«å®Œæ•´çš„åˆå§‹ usageï¼ŒåŒ…æ‹¬ç¼“å­˜ token
            const usage = chunk.message.usage;
            inputTokens = usage.input_tokens || 0;
            totalOutputTokens = usage.output_tokens || 0;
            // ç¼“å­˜ token åªåœ¨ message_start ä¸­å‡ºç°ï¼Œè®°å½•åä¸å†ç´¯åŠ 
            cacheCreationInputTokens = usage.cache_creation_input_tokens || 0;
            cacheReadInputTokens = usage.cache_read_input_tokens || 0;
          }
        } catch (e) {}
      }
    }
  } finally {
    reader.releaseLock();
  }
}

/**
 * ç»Ÿä¸€å…¥å£
 */
export async function* callCustomModelStream(
  modelConfig: CustomModelConfig,
  request: any,
  abortSignal?: AbortSignal
): AsyncGenerator<GenerateContentResponse> {
  console.log(`[CustomModel] Stream call: ${modelConfig.displayName} (${modelConfig.provider})`);
  if (modelConfig.provider === 'openai') yield* callOpenAICompatibleModelStream(modelConfig, request, abortSignal);
  else if (modelConfig.provider === 'anthropic') yield* callAnthropicModelStream(modelConfig, request, abortSignal);
  else throw new Error(`Unsupported custom model provider for streaming: ${modelConfig.provider}`);
}

export async function callCustomModel(
  modelConfig: CustomModelConfig,
  request: any,
  abortSignal?: AbortSignal
): Promise<GenerateContentResponse> {
  console.log(`[CustomModel] Unary call: ${modelConfig.displayName} (${modelConfig.provider})`);
  if (modelConfig.provider === 'openai') return callOpenAICompatibleModel(modelConfig, request, abortSignal);
  else if (modelConfig.provider === 'anthropic') return callAnthropicModel(modelConfig, request, abortSignal);
  else throw new Error(`Unsupported custom model provider: ${modelConfig.provider}`);
}

/**
 * @internal
 * å¯¼å‡º parseJSONSafe ç”¨äºå•å…ƒæµ‹è¯•
 * è¿™æ˜¯å†…éƒ¨å®ç°ç»†èŠ‚ï¼Œä¸å±äºå…¬å¼€ APIï¼Œå¯èƒ½éšæ—¶å˜æ›´
 */
export { parseJSONSafe as parseJSONSafeExport };
