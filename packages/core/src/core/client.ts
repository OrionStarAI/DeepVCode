/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import {
  GenerateContentConfig,
  Part,
  PartListUnion,
  Content,
  Tool,
  GenerateContentResponse,
} from '@google/genai';
import { getFolderStructure } from '../utils/getFolderStructure.js';
import { detectTerminalEnvironment, formatTerminalInfo } from '../utils/terminalDetection.js';
import { getNodeProcessTreeAsync, formatNodeProcessInfo } from '../utils/nodeProcessDetection.js';
import {
  Turn,
  ServerGeminiStreamEvent,
  GeminiEventType,
  ChatCompressionInfo,
  ModelSwitchResult,
} from './turn.js';
import { Config } from '../config/config.js';
import { UserTierId } from '../code_assist/types.js';
import { AgentContext } from '../telemetry/types.js';
import { getCoreSystemPrompt } from './prompts.js';
import { SceneType, SceneManager } from './sceneManager.js';
import { checkNextSpeaker } from '../utils/nextSpeakerChecker.js';
import { reportError } from '../utils/errorReporting.js';
import { GeminiChat } from './geminiChat.js';
import { getErrorMessage } from '../utils/errors.js';
import { tokenLimit } from './tokenLimits.js';
import {
  ContentGenerator,
  ContentGeneratorConfig,
  createContentGenerator,
} from './contentGenerator.js';
import { ProxyAgent, setGlobalDispatcher } from 'undici';
import { MESSAGE_ROLES } from '../config/messageRoles.js';
import { LoopDetectionService } from '../services/loopDetectionService.js';
import { CompressionService } from '../services/compressionService.js';
import { ideContext } from '../ide/ideContext.js';
import { logFlashDecidedToContinue } from '../telemetry/loggers.js';
import { FlashDecidedToContinueEvent, LoopType } from '../telemetry/types.js';
import { logger } from '../utils/enhancedLogger.js';

import { DeepVServerAdapter } from './DeepVServerAdapter.js';

function isThinkingSupported(model: string) {
  // âœ… æœåŠ¡ç«¯å†…éƒ¨å†³å®šæ¨¡å‹ - å®¢æˆ·ç«¯æ€»æ˜¯å°è¯•å¯ç”¨thinking
  // å¦‚æœæœåŠ¡ç«¯é€‰æ‹©çš„æ¨¡å‹ä¸æ”¯æŒï¼Œä¼šè¢«å¿½ç•¥ï¼Œä¸ä¼šå‡ºé”™
  return true; // è®©æœåŠ¡ç«¯å¤„ç†thinkingæ”¯æŒåˆ¤æ–­
}

// callGeminiEmbeddingAPI å‡½æ•°å·²ç§»é™¤ - åŠŸèƒ½æœªè¢«ä½¿ç”¨ä¸”å·²ä»æœåŠ¡ç«¯æ¸…ç†

/**
 * Returns the index of the content after the fraction of the total characters in the history.
 *
 * Exported for testing purposes.
 */
// ç§»é™¤ findIndexAfterFractionï¼Œç°åœ¨ä½¿ç”¨ CompressionService ä¸­çš„ç‰ˆæœ¬

export class GeminiClient {
  private chat?: GeminiChat;
  private contentGenerator?: ContentGenerator;
  private embeddingModel: string;
  private generateContentConfig: GenerateContentConfig = {
    temperature: 0,
    topP: 1,
  };
  private sessionTurnCount = 0;
  private readonly MAX_TURNS = 100;

  private readonly loopDetector: LoopDetectionService;
  private readonly compressionService: CompressionService;
  private lastPromptId?: string;
  private isCompressing: boolean = false; // å‹ç¼©äº’æ–¥é”ï¼Œé˜²æ­¢é‡å…¥

  // ä¸Šæ¬¡è¯·æ±‚çš„Tokenä½¿ç”¨é‡
  private sessionTokenCount: number = 0; //
  private compressionThreshold: number = 0.8; // åŠ¨æ€å‹ç¼©é˜ˆå€¼
  private readonly emergencyStopThreshold: number = 0.9; // ğŸš¨ ç´§æ€¥åˆ¶åŠ¨é˜ˆå€¼ï¼š90%
  private needsCompression: boolean = false; // æ˜¯å¦éœ€è¦åœ¨ä¸‹æ¬¡å¯¹è¯å‰å‹ç¼©

  constructor(private config: Config) {
    if (config.getProxy()) {
      setGlobalDispatcher(new ProxyAgent(config.getProxy() as string));
    }

    this.embeddingModel = config.getEmbeddingModel();
    this.loopDetector = new LoopDetectionService(config);

    //const compressionTokenThreshold = 0.8;
    this.compressionService = new CompressionService({
      compressionTokenThreshold: this.compressionThreshold,
      compressionPreserveThreshold: 0.3,
      skipEnvironmentMessages: 2, // è·³è¿‡ç¯å¢ƒä¿¡æ¯å’Œç¡®è®¤æ¶ˆæ¯
    });

    // åˆå§‹åŒ–æ™ºèƒ½å‹ç¼©é˜ˆå€¼ï¼ˆä½¿ç”¨ä¸CompressionServiceç›¸åŒçš„é€»è¾‘ï¼‰
    //this.compressionThreshold = compressionTokenThreshold * tokenLimit(this.config.getModel(), this.config);
  }

  async initialize(contentGeneratorConfig: ContentGeneratorConfig) {
    // ğŸª è§¦å‘ SessionStart é’©å­
    try {
      const { SessionStartSource } = await import('../hooks/types.js');
      await this.config.getHookSystem()
        .getEventHandler()
        .fireSessionStartEvent(SessionStartSource.Startup);
    } catch (hookError) {
      logger.warn(`[GeminiClient] SessionStart hook execution failed: ${hookError}`);
    }

    this.contentGenerator = await createContentGenerator(
      contentGeneratorConfig,
      this.config,
      this.config.getSessionId(),
    );
    this.chat = await this.startChat();
  }

  /**
   * ç»“æŸä¼šè¯å¹¶è§¦å‘ SessionEnd é’©å­
   */
  async endSession(reason: string = 'user_exit'): Promise<void> {
    try {
      const { SessionEndReason } = await import('../hooks/types.js');
      // æ˜ å°„å­—ç¬¦ä¸²åŸå› ä¸ºæšä¸¾
      let endReason = SessionEndReason.Exit;
      if (reason === 'error') endReason = SessionEndReason.Other;
      if (reason === 'timeout') endReason = SessionEndReason.Other;

      await this.config.getHookSystem()
        .getEventHandler()
        .fireSessionEndEvent(endReason);
    } catch (hookError) {
      logger.warn(`[GeminiClient] SessionEnd hook execution failed: ${hookError}`);
    }
  }

  getContentGenerator(): ContentGenerator {
    if (!this.contentGenerator) {
      throw new Error('Content generator not initialized');
    }
    return this.contentGenerator;
  }

  /**
   * è·å–é€šç”¨å†…å®¹ç”Ÿæˆå™¨
   * DeepVServerAdapter æ”¯æŒæ‰€æœ‰æ¨¡å‹ï¼šClaudeæ¨¡å‹è¿›è¡Œå‚æ•°è½¬æ¢ï¼ŒGeminiæ¨¡å‹ç›´æ¥è½¬å‘
   */
  private async getContentGeneratorForModel(model: string): Promise<ContentGenerator> {
    // åˆ›å»ºé€šç”¨é€‚é…å™¨ï¼Œæ”¯æŒClaudeå’ŒGeminiæ¨¡å‹
    const { hasAvailableProxyServer, getActiveProxyServerUrl } = await import('../config/proxyConfig.js');

    if (!hasAvailableProxyServer()) {
      throw new Error('DeepX Code server required for all models but is not available');
    }

    const proxyServerUrl = getActiveProxyServerUrl();
    // NOTE: googleCloudLocation and googleCloudProject are legacy parameters, no longer used after switching to proxy-based architecture
    const googleCloudLocation = process.env.GOOGLE_CLOUD_LOCATION || 'us-central1';
    const googleCloudProject = process.env.GOOGLE_CLOUD_PROJECT || 'default-project';

    return new DeepVServerAdapter(googleCloudLocation, googleCloudProject, proxyServerUrl, this.config);
  }

  /**
   * åˆ›å»ºä¸´æ—¶çš„ GeminiChat å®ä¾‹ç”¨äºå•æ¬¡å†…å®¹ç”Ÿæˆ
   * æä¾›å®Œæ•´çš„APIæ—¥å¿—ã€Tokenç»Ÿè®¡ã€é”™è¯¯å¤„ç†ç­‰åŠŸèƒ½
   *
   * @param scene ä½¿ç”¨åœºæ™¯ï¼Œç”¨äºé€‰æ‹©åˆé€‚çš„æ¨¡å‹
   * @param model å¯é€‰çš„ç‰¹å®šæ¨¡å‹ï¼Œä¼šè¦†ç›–åœºæ™¯æ¨èçš„æ¨¡å‹
   * @param agentContext ä»£ç†ä¸Šä¸‹æ–‡ï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„è°ƒç”¨æ¥æº
   * @returns ä¸´æ—¶ GeminiChat å®ä¾‹
   */
  async createTemporaryChat(
    scene: SceneType,
    model?: string,
    agentContext: AgentContext = { type: 'sub', agentId: SceneManager.getSceneDisplayName(scene) }
  ): Promise<GeminiChat> {
    const sceneModel = SceneManager.getModelForScene(scene);
    const modelToUse = model || sceneModel || this.config.getModel();

    // é€‰æ‹©åˆé€‚çš„å†…å®¹ç”Ÿæˆå™¨
    const contentGenerator = await this.getContentGeneratorForModel(modelToUse);

    // åˆ›å»ºç®€åŒ–çš„ç”Ÿæˆé…ç½®
    const userMemory = this.config.getUserMemory();
    const systemInstruction = getCoreSystemPrompt(userMemory);

    const isThinking = isThinkingSupported(modelToUse);
    const generateContentConfig = isThinking
      ? {
          ...this.generateContentConfig,
          thinkingConfig: {
            includeThoughts: false,
          },
        }
      : this.generateContentConfig;

    return new GeminiChat(
      this.config,
      contentGenerator,
      {
        systemInstruction,
        ...generateContentConfig,
        // æ— éœ€å·¥å…·å£°æ˜ï¼Œä¸´æ—¶chatä¸»è¦ç”¨äºç®€å•å†…å®¹ç”Ÿæˆ
      },
      [], // ç©ºå†å²ï¼Œä¸´æ—¶ä½¿ç”¨
      agentContext,
      modelToUse // ä¼ å…¥ç¡®å®šçš„æ¨¡å‹ï¼Œé¿å…è¢«configè¦†ç›–
    );
  }

  getUserTier(): UserTierId | undefined {
    return this.contentGenerator?.userTier;
  }

  async addHistory(content: Content) {
    this.getChat().addHistory(content);
  }

  getChat(): GeminiChat {
    if (!this.chat) {
      throw new Error('Chat not initialized');
    }
    return this.chat;
  }

  /**
   * æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¿›è¡Œå‹ç¼©æ“ä½œ
   * @returns å¦‚æœæ­£åœ¨å‹ç¼©è¿”å›trueï¼Œå¦åˆ™è¿”å›false
   */
  isCompressionInProgress(): boolean {
    return this.isCompressing;
  }

  /**
   * å¤„ç†å“åº”åçš„tokenæ›´æ–°å’Œå‹ç¼©å†³ç­–
   * @param inputTokens è¾“å…¥tokenæ•°é‡
   * @param outputTokens è¾“å‡ºtokenæ•°é‡
   */
  private updateTokenCountAndCheckCompression(inputTokens: number, outputTokens: number): void {
    this.sessionTokenCount = inputTokens + outputTokens;

    let compressionTokenThreshold = this.compressionThreshold * tokenLimit(this.config.getModel(), this.config);
    // æ£€æŸ¥æ˜¯å¦è¶…è¿‡å‹ç¼©é˜ˆå€¼
    if (this.sessionTokenCount >= compressionTokenThreshold) {
      this.needsCompression = true;
      logger.info(`[GeminiClient] Token threshold reached: ${this.sessionTokenCount} >= ${this.compressionThreshold}, scheduling compression for next conversation`);
    }
  }

  // åˆ‡æ¢æ¨¡å‹çš„è¯ï¼Œéœ€è¦å†æ¬¡æ£€æµ‹å‹ç¼©é˜ˆå€¼
  private checkCompression(): void {
    if (!this.needsCompression) {
      let compressionTokenThreshold = this.compressionThreshold * tokenLimit(this.config.getModel(), this.config);
      if (this.sessionTokenCount >= compressionTokenThreshold) {
        this.needsCompression = true;
        logger.info(`[GeminiClient] Token threshold reached: ${this.sessionTokenCount} >= ${this.compressionThreshold}, scheduling compression for next conversation`);
      }
    }
  }

  /**
   * é‡ç½®å‹ç¼©æ ‡è®°ï¼ˆåœ¨å‹ç¼©å®Œæˆåè°ƒç”¨ï¼‰
   */
  private resetCompressionFlag(): void {
    this.needsCompression = false;
    // å‹ç¼©åé‡ç½®tokenè®¡æ•°å™¨ï¼Œå› ä¸ºå†å²å·²ç»è¢«å‹ç¼©
    this.sessionTokenCount = 0;
  }

  /**
   * ç­‰å¾…å‹ç¼©å®Œæˆ
   * @param abortSignal ç”¨äºå–æ¶ˆç­‰å¾…çš„ä¿¡å·
   * @param maxWaitMs æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
   */
  private async waitForCompressionComplete(abortSignal?: AbortSignal): Promise<void> {
    if (!this.isCompressing) {
      return; // æ²¡æœ‰åœ¨å‹ç¼©ï¼Œç›´æ¥è¿”å›
    }
    const pollInterval = 100; // 100ms è½®è¯¢é—´éš”

    while (this.isCompressing) {
      // æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
      if (abortSignal?.aborted) {
        break;
      }
      // ç­‰å¾…ä¸€å°æ®µæ—¶é—´åå†æ£€æŸ¥
      await new Promise(resolve => setTimeout(resolve, pollInterval));
    }
  }

  isInitialized(): boolean {
    return this.chat !== undefined && this.contentGenerator !== undefined;
  }

  getHistory(): Content[] {
    return this.getChat().getHistory();
  }

  setHistory(history: Content[]) {
    this.getChat().setHistory(history);
  }

  async setTools(): Promise<void> {
    const toolRegistry = await this.config.getToolRegistry();
    const toolDeclarations = toolRegistry.getFunctionDeclarations();
    const tools: Tool[] = [{ functionDeclarations: toolDeclarations }];
    this.getChat().setTools(tools);
  }

  async resetChat(): Promise<void> {
    this.resetCompressionFlag();
    this.chat = await this.startChat();
  }

  private async getEnvironment(): Promise<Part[]> {
    const cwd = this.config.getWorkingDir();
    const today = new Date().toLocaleDateString(undefined, {
      weekday: 'long',
      year: 'numeric',
      month: 'long',
      day: 'numeric',
    });

    // å¼‚æ­¥æ£€æµ‹ç¯å¢ƒï¼Œä¸é˜»å¡åˆå§‹åŒ–
    let environmentInfo = '';
    let nodeProcessInfo = '';
    try {
      // ä½¿ç”¨ setTimeout è®©ç¯å¢ƒæ£€æµ‹å¼‚æ­¥è¿›è¡Œï¼Œé¿å…é˜»å¡UI
      const terminalInfo = await new Promise<any>((resolve) => {
        setTimeout(() => {
          try {
            const result = detectTerminalEnvironment();
            resolve(result);
          } catch (error) {
            console.warn('[Environment Detection] æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€ä¿¡æ¯:', error);
            resolve({
              platform: process.platform,
              shell: 'Unknown',
              terminal: 'Unknown'
            });
          }
        }, 0);
      });
      environmentInfo = formatTerminalInfo(terminalInfo);

      // æ£€æµ‹VSCodeç¯å¢ƒï¼Œå†³å®šæ˜¯å¦è·³è¿‡è¿›ç¨‹æ£€æµ‹
      const isVSCodeEnvironment = this.config.getVsCodePluginMode();

      // æ£€æµ‹Node.jsè¿›ç¨‹æ ‘ä¿¡æ¯ - ä½¿ç”¨æ–°çš„å¼‚æ­¥æ£€æµ‹æ–¹æ³•ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
      // åœ¨VSCodeæ’ä»¶ç¯å¢ƒä¸­ï¼Œè·³è¿‡å¤æ‚çš„è¿›ç¨‹æ£€æµ‹ä»¥é¿å…CLIè‡ªæ€é£é™©
      const nodeProcesses = await Promise.race([
        getNodeProcessTreeAsync(isVSCodeEnvironment), // ä¼ é€’VSCodeç¯å¢ƒå‚æ•°
        new Promise<any[]>((_, reject) =>
          setTimeout(() => reject(new Error('Process detection timeout')), 5000)
        )
      ]).catch((error) => {
        console.warn('[Process Detection] å¼‚æ­¥æ£€æµ‹è¶…æ—¶æˆ–å¤±è´¥ï¼Œä½¿ç”¨åŒæ­¥å›é€€:', error);
        return [{
          pid: process.pid,
          ppid: process.ppid || 0,
          name: 'node',
          commandLine: process.argv.join(' ')
        }];
      });

      nodeProcessInfo = await Promise.race([
        formatNodeProcessInfo(nodeProcesses),
        new Promise<string>((_, reject) =>
          setTimeout(() => reject(new Error('Format timeout')), 2000)
        )
      ]).catch((error) => {
        console.warn('[Process Info Format] æ ¼å¼åŒ–è¶…æ—¶ï¼Œä½¿ç”¨åŸºç¡€ä¿¡æ¯:', error);
        return `Current process PID: ${process.pid} (Node.js CLI - do not kill)`;
      });
    } catch (error) {
      console.warn('[Environment Detection] ç¯å¢ƒä¿¡æ¯è·å–å¤±è´¥:', error);
      environmentInfo = `My operating system: ${process.platform}`;
      nodeProcessInfo = `Current process PID: ${process.pid} (Node.js CLI - do not kill)`;
    }

    // ä¼˜åŒ–ï¼šä½¿ç”¨æ›´ç®€æ´çš„é¡¹ç›®ç»“æ„ä¿¡æ¯ï¼Œé¿å…åˆå§‹ä¸Šä¸‹æ–‡è¿‡å¤§
    const folderStructure = await getFolderStructure(cwd, {
      fileService: this.config.getFileService(),
      fileIncludePattern: /\.(ts|js|tsx|jsx|json|md|py|go|rs|java|cpp|c|h|yml|yaml|toml)$/i, // åªæ˜¾ç¤ºé‡è¦æ–‡ä»¶ç±»å‹
    });

    const context = `
ğŸš€ **CRITICAL SYSTEM CONTEXT - DeepV Code AI Assistant** ğŸš€
This is the DeepV Code CLI with enhanced environment awareness.
**Date:** ${today}
**Platform:** ${environmentInfo}
**ğŸ¯ CRITICAL: Always use ${process.platform}-appropriate commands!**
**Working Directory:** ${cwd}
${nodeProcessInfo}

**ğŸ“ PROJECT STRUCTURE:**
${folderStructure}

**ğŸ› ï¸ AVAILABLE TOOLS:**
Use Glob and ReadFile tools to explore specific files during our conversation.

**ğŸ”’ SAFETY REMINDERS:**
- Respect the process hierarchy shown above
- Always explain potentially destructive commands before execution
- Consider cross-platform compatibility in all suggestions
          `.trim();

    const initialParts: Part[] = [{ text: context }];
    const toolRegistry = await this.config.getToolRegistry();

    // ğŸš€ æ™ºèƒ½FullContextåŠŸèƒ½ï¼šä½¿ç”¨ä¼˜åŒ–åçš„ReadManyFilesTool
    if (this.config.getFullContext()) {
      try {
        const readManyFilesTool = toolRegistry.getTool('read_many_files');
        if (readManyFilesTool) {
          console.log('ğŸ” Loading full context with intelligent content management...');

          // ä½¿ç”¨æ™ºèƒ½ReadManyFilesToolè¯»å–é¡¹ç›®æ–‡ä»¶
          const result = await readManyFilesTool.execute({
            paths: ['**/*'], // è¯»å–æ‰€æœ‰æ–‡ä»¶
            useDefaultExcludes: true, // ä½¿ç”¨é»˜è®¤æ’é™¤è§„åˆ™
            exclude: [
              // é¢å¤–æ’é™¤ä¸€äº›å¯èƒ½å¾ˆå¤§çš„æ–‡ä»¶ç±»å‹
              '**/*.log',
              '**/*.tmp',
              '**/*.lock',
              '**/package-lock.json',
              '**/yarn.lock',
              '**/pnpm-lock.yaml',
            ]
          }, AbortSignal.timeout(30000));

          if (result.llmContent && Array.isArray(result.llmContent) && result.llmContent.length > 0) {
            // è®¡ç®—å†…å®¹å¤§å°æ¥éªŒè¯æˆ‘ä»¬çš„é™åˆ¶æœºåˆ¶æ˜¯å¦ç”Ÿæ•ˆ
            const contentSize = JSON.stringify(result.llmContent).length;
            console.log(`ğŸ“Š Full context loaded: ${Math.round(contentSize / 1024)}KB (with intelligent limits applied)`);

            initialParts.push({
              text: `\n--- ğŸš€ Full Project Context (Intelligently Managed) ---\n${result.llmContent}`,
            });
          } else {
            console.warn('âš ï¸ Full context requested, but read_many_files returned no content.');
            initialParts.push({
              text: '\n--- â„¹ï¸ Full context requested but no files found ---',
            });
          }
        } else {
          console.warn('âš ï¸ Full context requested, but read_many_files tool not available.');
          initialParts.push({
            text: '\n--- âš ï¸ Full context unavailable: read_many_files tool not found ---',
          });
        }
      } catch (error) {
        console.error('âŒ Error loading full context:', error);
        initialParts.push({
          text: '\n--- âŒ Error loading full context: Content limits may have been exceeded ---',
        });
      }
    }

    return initialParts;
  }

  async startChat(extraHistory?: Content[], agentContext?: AgentContext): Promise<GeminiChat> {
    const envParts = await this.getEnvironment();
    const toolRegistry = await this.config.getToolRegistry();
    const toolDeclarations = toolRegistry.getFunctionDeclarations();
    const tools: Tool[] = [{ functionDeclarations: toolDeclarations }];
    const history: Content[] = [
      {
        role: MESSAGE_ROLES.USER,
        parts: envParts,
      },
      {
        role: MESSAGE_ROLES.MODEL,
        parts: [{ text: 'Got it. Thanks for the context!' }],
      },
      ...(extraHistory ?? []),
    ];
    try {
      const userMemory = this.config.getUserMemory();

      // æ£€æŸ¥æ˜¯å¦ä¸ºVSCodeç¯å¢ƒ
      const isVSCode = this.config.getVsCodePluginMode();

      // ä½¿ç”¨æ–°çš„getCoreSystemPromptï¼Œæ ¹æ®ç¯å¢ƒè°ƒæ•´å†…å®¹
      const systemInstruction = getCoreSystemPrompt(userMemory, isVSCode);

      const generateContentConfigWithThinking = isThinkingSupported(
        this.config.getModel(),
      )
        ? {
            ...this.generateContentConfig,
            thinkingConfig: {
              includeThoughts: false,
            },
          }
        : this.generateContentConfig;
      return new GeminiChat(
        this.config,
        this.getContentGenerator(),
        {
          systemInstruction,
          ...generateContentConfigWithThinking,
          tools,
        },
        history,
        agentContext || { type: 'main' }, // é»˜è®¤ä¸ºä¸»ä¼šè¯
        this.config.getModel() // ä¸»ä¼šè¯ä½¿ç”¨é…ç½®çš„é»˜è®¤æ¨¡å‹
      );
    } catch (error) {
      await reportError(
        error,
        'Error initializing Gemini chat session.',
        history,
        'startChat',
      );
      throw new Error(`Failed to initialize chat: ${getErrorMessage(error)}`);
    }
  }

  async *sendMessageStream(
    request: PartListUnion,
    signal: AbortSignal,
    prompt_id: string,
    turns: number = this.MAX_TURNS,
    originalModel?: string,
  ): AsyncGenerator<ServerGeminiStreamEvent, Turn> {
    // ğŸª è§¦å‘ BeforeAgent é’©å­
    try {
      const beforeAgentResult = await this.config.getHookSystem()
        .getEventHandler()
        .fireBeforeAgentEvent(JSON.stringify(request));

      // æ£€æŸ¥é’©å­æ˜¯å¦é˜»æ­¢æ‰§è¡Œ
      if (beforeAgentResult?.finalOutput?.shouldStopExecution?.()) {
        yield {
          type: GeminiEventType.Error,
          value: {
            error: {
              message: `Agent execution blocked by BeforeAgent hook`
            }
          }
        } as any;
        return new Turn(this.getChat(), prompt_id, this.config.getModel(), this.config);
      }
    } catch (hookError) {
      logger.warn(`[GeminiClient] BeforeAgent hook execution failed: ${hookError}`);
    }

    if (this.lastPromptId !== prompt_id) {
      this.loopDetector.reset(prompt_id);
      this.lastPromptId = prompt_id;
    }
    this.sessionTurnCount++;
    if (
      this.config.getMaxSessionTurns() > 0 &&
      this.sessionTurnCount > this.config.getMaxSessionTurns()
    ) {
      yield { type: GeminiEventType.MaxSessionTurns };
      return new Turn(this.getChat(), prompt_id, this.config.getModel(), this.config);
    }
    // Ensure turns never exceeds MAX_TURNS to prevent infinite loops
    const boundedTurns = Math.min(turns, this.MAX_TURNS);
    if (!boundedTurns) {
      return new Turn(this.getChat(), prompt_id, this.config.getModel());
    }

    // Track the original model from the first call to detect model switching
    const initialModel = originalModel || this.config.getModel();

    // ğŸ”§ æ£€æŸ¥å¹¶è¡¥å…¨æœªå®Œæˆçš„ function call
    //this.handleIncompleteFunctionCall(request);

    // å¦‚æœæ­£åœ¨å‹ç¼©ï¼Œç­‰å¾…å‹ç¼©å®Œæˆä»¥ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
    if (this.isCompressing) {
      console.log('[sendMessageStream] Waiting for ongoing compression to complete...');
      await this.waitForCompressionComplete(signal);
      console.log('[sendMessageStream] Compression wait completed, proceeding');
    }


    this.checkCompression();
    // åŸºäºå“åº”çš„æ™ºèƒ½å‹ç¼©ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦åœ¨æœ¬æ¬¡å¯¹è¯å‰è¿›è¡Œå‹ç¼©
    // åªæœ‰å½“ needsCompression æ ‡è®°ä¸º true æ—¶æ‰å°è¯•å‹ç¼©ï¼Œå¦åˆ™ä¸è§¦å‘å‹ç¼©æµç¨‹å’Œ PreCompress é’©å­
    if (this.needsCompression) {
      console.log('[sendMessageStream] Token threshold exceeded, performing compression before new conversation');
      const compressed = await this.tryCompressChat(prompt_id, signal, true); // å¼ºåˆ¶å‹ç¼©
      if (compressed) {
        yield { type: GeminiEventType.ChatCompressed, value: compressed };
        this.resetCompressionFlag(); // å‹ç¼©å®Œæˆåé‡ç½®æ ‡è®°
      } else {
        console.warn('[sendMessageStream] Failed to perform scheduled compression');
      }
    }

    // æ£€æŸ¥requestæ˜¯å¦åŒ…å«function responseï¼Œå¦‚æœåŒ…å«åˆ™è·³è¿‡IDEä¸Šä¸‹æ–‡ä¿¡æ¯
    const requestParts = Array.isArray(request) ? request : [request];
    const hasFunctionResponse = requestParts.some(part => {
      if (typeof part === 'string') return false;
      return !!part.functionResponse;
    });

    if (this.config.getIdeMode() && !hasFunctionResponse) {
      const openFiles = ideContext.getOpenFilesContext();
      if (openFiles) {
        const contextParts: string[] = [];
        if (openFiles.activeFile) {
          contextParts.push(
            `This is the file that the user was most recently looking at:\n- Path: ${openFiles.activeFile}`,
          );
          if (openFiles.cursor) {
            contextParts.push(
              `This is the cursor position in the file:\n- Cursor Position: Line ${openFiles.cursor.line}, Character ${openFiles.cursor.character}`,
            );
          }
          if (openFiles.selectedText) {
            contextParts.push(
              `This is the selected text in the active file:\n- ${openFiles.selectedText}`,
            );
          }
        }

        if (openFiles.recentOpenFiles && openFiles.recentOpenFiles.length > 0) {
          const recentFiles = openFiles.recentOpenFiles
            .map((file) => `- ${file.filePath}`)
            .join('\n');
          contextParts.push(
            `Here are files the user has recently opened, with the most recent at the top:\n${recentFiles}`,
          );
        }

        if (contextParts.length > 0) {
          request = [
            { text: contextParts.join('\n') },
            ...(Array.isArray(request) ? request : [request]),
          ];
        }
      }
    }

    const turn = new Turn(this.getChat(), prompt_id, this.config.getModel());

    const loopDetected = await this.loopDetector.turnStarted(signal);
    if (loopDetected) {
      const loopType = this.loopDetector.getDetectedLoopType();
      yield { type: GeminiEventType.LoopDetected, value: loopType ? loopType.toString() : undefined };
      // Add feedback to chat history so AI understands why it was stopped
      this.addLoopDetectionFeedbackToHistory(loopType);
      return turn;
    }

    const resultStream = turn.run(request, signal);
    for await (const event of resultStream) {
      if (this.loopDetector.addAndCheck(event)) {
        const loopType = this.loopDetector.getDetectedLoopType();
        yield { type: GeminiEventType.LoopDetected, value: loopType ? loopType.toString() : undefined };
        // Add feedback to chat history so AI understands why it was stopped
        this.addLoopDetectionFeedbackToHistory(loopType);
        return turn;
      }

      // å¤„ç†TokenUsageäº‹ä»¶ï¼Œç´¯ç§¯tokenè®¡æ•°å¹¶åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸‹æ¬¡å‹ç¼©
      if (event.type === GeminiEventType.TokenUsage) {
        const tokenInfo = event.value;
        this.updateTokenCountAndCheckCompression(
          tokenInfo.inputTokens,
          tokenInfo.outputTokens
        );

        // ç»§ç»­ä¼ é€’äº‹ä»¶ç»™ä¸Šå±‚å¤„ç†
        yield event;
      } else {
        yield event;
      }
    }
    if (!turn.pendingToolCalls.length && signal && !signal.aborted) {
      // Check if model was switched during the call (likely due to quota error)
      const currentModel = this.config.getModel();
      if (currentModel !== initialModel) {
        // Model was switched (likely due to quota error fallback)
        // Don't continue with recursive call to prevent unwanted Flash execution
        return turn;
      }

      const nextSpeakerCheck = await checkNextSpeaker(
        this.getChat(),
        this,
        signal,
      );
      if (nextSpeakerCheck?.next_speaker === 'model') {
        logFlashDecidedToContinue(
          this.config,
          new FlashDecidedToContinueEvent(prompt_id),
        );
        const nextRequest = [{ text: 'Please continue.' }];
        // This recursive call's events will be yielded out, and the final
        // turn object will be from the recursive call.
        return yield* this.sendMessageStream(
          nextRequest,
          signal,
          prompt_id,
          boundedTurns - 1,
          initialModel,
        );
      }
    }

    // ğŸª è§¦å‘ AfterAgent é’©å­ - åœ¨æ¯ä¸ª turn å®Œæˆåæ‰§è¡Œï¼ˆæŒ‰ç…§åŸç‰ˆé€»è¾‘ï¼‰
    try {
      const responses = turn.getDebugResponses();
      const lastResponse = responses.length > 0 ? responses[responses.length - 1] : {};

      await this.config.getHookSystem()
        .getEventHandler()
        .fireAfterAgentEvent(
          JSON.stringify(request),
          JSON.stringify(lastResponse),
          false
        );
    } catch (hookError) {
      logger.warn(`[GeminiClient] AfterAgent hook execution failed: ${hookError}`);
    }

    return turn;
  }

  // generateEmbedding æ–¹æ³•å·²ç§»é™¤ - åŠŸèƒ½æœªè¢«ä½¿ç”¨ä¸”å·²ä»æœåŠ¡ç«¯æ¸…ç†

  async tryCompressChat(
    prompt_id: string,
    abortSignal: AbortSignal,
    force: boolean = false,
  ): Promise<ChatCompressionInfo | null> {
    // æ£€æŸ¥å‹ç¼©é”ï¼Œé˜²æ­¢é‡å…¥
    if (this.isCompressing) {
      console.warn('[tryCompressChat] Compression already in progress, skipping');
      return null;
    }

    // è®¾ç½®å‹ç¼©é”
    this.isCompressing = true;

    try {
      // ğŸª è§¦å‘ PreCompress é’©å­
      try {
        const { PreCompressTrigger } = await import('../hooks/types.js');
        await this.config.getHookSystem()
          .getEventHandler()
          .firePreCompressEvent(
            force ? PreCompressTrigger.Manual : PreCompressTrigger.Auto
          );
      } catch (hookError) {
        logger.warn(`[GeminiClient] PreCompress hook execution failed: ${hookError}`);
      }

      const curatedHistory = this.getChat().getHistory(true);
      let compressionModel = SceneManager.getModelForScene(SceneType.COMPRESSION);

      // ğŸš€ Dynamic Model Upgrade: If current token count exceeds Flash's limit (~1M),
      // upgrade to x-ai/grok-4.1-fast to ensure compression succeeds.
      // Using 900,000 as a safe threshold to allow buffer for output and overhead.
      if (this.sessionTokenCount > 900000) {
        console.log(`[tryCompressChat] Token count (${this.sessionTokenCount}) exceeds Flash limit. Upgrading compression model to x-ai/grok-4.1-fast.`);
        compressionModel = 'x-ai/grok-4.1-fast';
      }

      const historyModel = this.config.getModel(); // historyå®é™…ä½¿ç”¨çš„æ¨¡å‹ï¼Œç”¨äºæµ‹ç®—é•¿åº¦

      // ä½¿ç”¨å‹ç¼©æœåŠ¡
      const compressionResult = await this.compressionService.tryCompress(
        this.config,
        curatedHistory,
        historyModel!,
        compressionModel!,
        this, // ä¼ é€’ GeminiClient å®ä¾‹è€Œä¸æ˜¯ ContentGenerator
        prompt_id,
        abortSignal,
        force
      );

      if (!compressionResult || !compressionResult.success) {
        if (compressionResult?.error) {
          console.warn(`[GeminiClient] Compression failed: ${compressionResult.error}`);
        }
        return null;
      }

      // åº”ç”¨å‹ç¼©ç»“æœï¼šç›´æ¥è®¾ç½®æ–°çš„å†å²è®°å½•
      if (compressionResult.newHistory) {
        this.getChat().setHistory(compressionResult.newHistory);
        console.log('[tryCompressChat] Compression applied successfully');
      }

      return compressionResult.compressionInfo || null;
    } finally {
      // ç¡®ä¿å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿèƒ½é‡Šæ”¾é”
      this.isCompressing = false;
    }
  }

  /**
   * åˆ‡æ¢æ¨¡å‹å¹¶ç¡®ä¿ä¸Šä¸‹æ–‡å®‰å…¨
   *
   * æ­¤æ–¹æ³•åœ¨åˆ‡æ¢æ¨¡å‹å‰æ£€æŸ¥å½“å‰å†å²æ˜¯å¦é€‚åº”æ–°æ¨¡å‹çš„ä¸Šä¸‹æ–‡é™åˆ¶ã€‚
   * å¦‚æœè¶…å‡ºé™åˆ¶ï¼Œä¼šå°è¯•è¿›è¡Œæ¿€è¿›å‹ç¼©ã€‚
   *
   * @param newModel ç›®æ ‡æ¨¡å‹åç§°
   * @param abortSignal ä¸­æ­¢ä¿¡å·
   * @param knownTokenCount å¯é€‰çš„å·²çŸ¥tokenæ•°é‡ï¼ˆç”±è°ƒç”¨æ–¹æä¾›ï¼Œé¿å…é‡æ–°è®¡ç®—ï¼‰
   * @returns åˆ‡æ¢ç»“æœï¼ŒåŒ…å«æˆåŠŸçŠ¶æ€å’Œå‹ç¼©ä¿¡æ¯
   */
  async switchModel(newModel: string, abortSignal: AbortSignal, knownTokenCount?: number): Promise<ModelSwitchResult> {
    if (this.isCompressing) {
      console.warn('[switchModel] Compression in progress, cannot switch model now.');
      return {
        success: false,
        modelName: newModel,
        error: 'Compression in progress, cannot switch model now.'
      };
    }

    const currentModel = this.config.getModel();
    if (currentModel === newModel) {
      return { success: true, modelName: newModel };
    }

    console.log(`[switchModel] Attempting to switch from ${currentModel} to ${newModel}...`);

    // è®¾ç½®å‹ç¼©é”
    this.isCompressing = true;

    try {
      const curatedHistory = this.getChat().getHistory(true);
      let compressionModel = SceneManager.getModelForScene(SceneType.COMPRESSION);

      // ğŸš€ Dynamic Model Upgrade: If current token count exceeds Flash's limit (~1M),
      // upgrade to x-ai/grok-4.1-fast to ensure compression succeeds.
      // Using 900,000 as a safe threshold to allow buffer for output and overhead.
      // We use sessionTokenCount as a proxy, or we could recount if needed.
      if (this.sessionTokenCount > 900000) {
        console.log(`[switchModel] Token count (${this.sessionTokenCount}) exceeds Flash limit. Upgrading compression model to x-ai/grok-4.1-fast.`);
        compressionModel = 'x-ai/grok-4.1-fast';
      }

      // å°è¯•å‹ç¼©ä»¥é€‚åº”æ–°æ¨¡å‹
      const compressionResult = await this.compressionService.compressToFit(
        this.config,
        curatedHistory,
        currentModel,
        newModel,
        compressionModel!,
        this,
        `switch-model-${Date.now()}`,
        abortSignal,
        knownTokenCount
      );

      const modelSwitchResult: ModelSwitchResult = {
        success: true,
        modelName: newModel
      };

      console.log(`[switchModel] compressionResult:`, {
        success: compressionResult?.success,
        hasSkipReason: !!compressionResult?.skipReason,
        hasCompressionInfo: !!compressionResult?.compressionInfo,
        hasNewHistory: !!compressionResult?.newHistory,
        hasError: !!compressionResult?.error
      });

      if (compressionResult.skipReason) {
        // ä¸éœ€è¦å‹ç¼©ï¼Œæ˜¾ç¤ºåŸå› 
        console.log(`[switchModel] ${compressionResult.skipReason}`);
        modelSwitchResult.compressionSkipReason = compressionResult.skipReason;
      } else if (compressionResult.success && compressionResult.newHistory) {
        // å‹ç¼©æˆåŠŸ
        this.getChat().setHistory(compressionResult.newHistory);
        if (compressionResult.compressionInfo) {
          console.log(
            `[switchModel] History compressed to fit new model: ` +
            `${compressionResult.compressionInfo.originalTokenCount} â†’ ` +
            `${compressionResult.compressionInfo.newTokenCount} tokens`
          );
          modelSwitchResult.compressionInfo = compressionResult.compressionInfo;
        } else {
          console.log('[switchModel] History compressed to fit new model.');
        }
      } else {
        console.warn(`[switchModel] Compression failed: ${compressionResult.error}`);
        modelSwitchResult.success = false;
        modelSwitchResult.error = compressionResult.error;
        // å‹ç¼©å¤±è´¥ï¼Œé˜»æ­¢åˆ‡æ¢
        this.isCompressing = false;
        return modelSwitchResult;
      }

      // æ›´æ–°é…ç½®å’ŒChat
      this.config.setModel(newModel);
      this.getChat().setSpecifiedModel(newModel);

      // é‡ç½®å‹ç¼©æ ‡è®°ï¼Œå› ä¸ºä¸Šä¸‹æ–‡å¯èƒ½å·²ç»æ”¹å˜
      this.resetCompressionFlag();

      console.log(`[switchModel] Successfully switched to ${newModel}`);
      return modelSwitchResult;

    } catch (error) {
      console.error('[switchModel] Error during model switch:', error);
      return {
        success: false,
        modelName: newModel,
        error: error instanceof Error ? error.message : String(error)
      };
    } finally {
      this.isCompressing = false;
    }
  }

  /**
   * å¾ªç¯æ£€æµ‹è§¦å‘æ—¶ï¼Œå‘å†å²ä¸­æ·»åŠ ç»™ AI çš„åé¦ˆä¿¡æ¯
   * è¿™æ · AI èƒ½ç†è§£ä¸ºä»€ä¹ˆè¢«ä¸­æ­¢ï¼Œä»¥åŠåº”è¯¥å¦‚ä½•æ”¹è¿›
   */
  private addLoopDetectionFeedbackToHistory(loopType: LoopType | null): void {
    let feedbackMessage = '';

    switch (loopType) {
      case LoopType.CONSECUTIVE_IDENTICAL_TOOL_CALLS:
        feedbackMessage = `ğŸ”´ LOOP DETECTED: You were repeatedly calling the same tool, which wastes context and API quota.

âš ï¸ Why this happened:
â€¢ You may be stuck in the same approach
â€¢ The current direction is not productive
â€¢ Missing or unclear task context

âœ… What to do next:
1. Review the task: Was the original request clear enough?
2. Take a different approach: Try exploring from a different angle
3. Ask for clarification: Request more specific guidance or context
4. Example: Instead of reading many files, focus on specific files mentioned in the error or task

ğŸ’¡ Tips:
â€¢ Break complex tasks into smaller, focused subtasks
â€¢ Be explicit about what you're trying to achieve
â€¢ When stuck, ask for hints or a different approach`;
        break;

      case LoopType.CHANTING_IDENTICAL_SENTENCES:
        feedbackMessage = `ğŸ”´ LOOP DETECTED: You were repeatedly generating the same text, which indicates being stuck.

âš ï¸ Why this happened:
â€¢ The model may be stuck on a specific pattern or thought
â€¢ Unable to progress beyond a certain point
â€¢ May need external guidance to break the pattern

âœ… What to do next:
1. Acknowledge the issue: Understand what went wrong
2. Take a fresh approach: Try a completely different angle
3. Ask for help: Request guidance on how to proceed differently
4. Example: If stuck explaining something, ask to try a different explanation method`;
        break;

      case LoopType.LLM_DETECTED_LOOP:
        feedbackMessage = `ğŸ”´ LOOP DETECTED: The AI analysis detected that you're not making meaningful progress.

âš ï¸ Why this happened:
â€¢ The current approach is not advancing the task
â€¢ May be exploring unproductive paths
â€¢ Need to refocus on the core objective

âœ… What to do next:
1. Clarify the goal: Restate what needs to be accomplished
2. Provide constraints: Give clear boundaries or requirements
3. Break it down: Divide into smaller, achievable steps
4. Change direction: Try a fundamentally different approach`;
        break;

      default:
        feedbackMessage = `ğŸ”´ LOOP DETECTED: The conversation entered a repetitive loop without making progress.

âœ… What to do next:
â€¢ Provide more specific guidance or constraints
â€¢ Clarify what you're trying to achieve
â€¢ Try a different approach to the problem
â€¢ Start fresh with /session new if needed`;
    }

    // æ·»åŠ åˆ°å†å²è®°å½•ä¸­ï¼Œæ ‡è®°ä¸ºç”¨æˆ·æ¶ˆæ¯
    this.getChat().addHistory({
      role: MESSAGE_ROLES.USER,
      parts: [{ text: feedbackMessage }],
    });
  }

  /**
   * å½“è¾¾åˆ° 90% Token é™åˆ¶æ—¶ï¼Œå‘å†å²è®°å½•æ·»åŠ åé¦ˆ
   */
  private addContextLimitFeedbackToHistory(): void {
    const feedbackMessage = `ğŸ›‘ EMERGENCY STOP: Context limit reached (90%).

âš ï¸ Execution has been paused to prevent context overflow.
The system will now compress the conversation history to free up space.

âœ… What happens next:
1. The context will be compressed automatically.
2. You can continue your task with the compressed history.
3. Please summarize your current progress and next steps after compression.`;

    this.getChat().addHistory({
      role: MESSAGE_ROLES.USER,
      parts: [{ text: feedbackMessage }],
    });
  }

}
